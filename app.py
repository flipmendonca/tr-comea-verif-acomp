import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import re
import string
from wordcloud import WordCloud
import warnings
from datetime import datetime, timedelta
from sklearn.metrics.pairwise import cosine_similarity
# Importar AgGrid para tabelas interativas
from st_aggrid import AgGrid, GridOptionsBuilder, GridUpdateMode, DataReturnMode
import os
warnings.filterwarnings('ignore')

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="TR | COMEA - An√°lise de Acompanhamentos",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Baixar recursos necess√°rios do NLTK
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('punkt')
    nltk.download('stopwords')

# Fun√ß√£o para pr√©-processamento de texto
def preprocessar_texto(texto):
    if pd.isna(texto):
        return ""
    # Converter para min√∫sculas
    texto = texto.lower()
    # Remover pontua√ß√£o
    texto = texto.translate(str.maketrans('', '', string.punctuation))
    # Remover n√∫meros
    texto = re.sub(r'\d+', '', texto)
    # Remover stopwords
    stop_words = set(stopwords.words('portuguese'))
    tokens = word_tokenize(texto)
    tokens = [word for word in tokens if word not in stop_words]
    # Reunir os tokens
    texto_processado = ' '.join(tokens)
    return texto_processado

# Fun√ß√£o para detectar problemas nas descri√ß√µes
def detectar_problemas(texto, palavras_vagas=None):
    if pd.isna(texto):
        return ['texto_nulo']
    
    if not isinstance(texto, str):
        return ['texto_invalido']
    
    problemas = []
    
    # Verificar descri√ß√µes muito curtas
    if len(texto) < 20:
        problemas.append('muito_curto')
    
    if palavras_vagas is None:
        palavras_vagas = ['etc', 'outros', 'algumas', 'alguma coisa', 'algo', 'coisas', 'demandas', 'entre outros']
    
    # Verificar termos vagos
    for palavra in palavras_vagas:
        if palavra in texto.lower():
            problemas.append('vago')
            break
    
    # Verificar falta de detalhes espec√≠ficos
    if len(texto.split()) < 10:
        problemas.append('sem_detalhes')
    
    # Verificar descri√ß√µes gen√©ricas
    frases_genericas = [
        'atendimento realizado', 
        'demanda atendida', 
        'feito contato', 
        'realizado acompanhamento',
        'foi feito'
    ]
    
    for frase in frases_genericas:
        if frase in texto.lower() and len(texto) < 50:
            problemas.append('generico')
            break
    
    return problemas

# Fun√ß√£o para classificar a qualidade da descri√ß√£o
def classificar_qualidade(texto):
    problemas = detectar_problemas(texto)
    
    if 'texto_nulo' in problemas or 'texto_invalido' in problemas:
        return 'Cr√≠tico', 0
    
    if 'muito_curto' in problemas:
        return 'Cr√≠tico', 1
    
    if 'generico' in problemas and 'sem_detalhes' in problemas:
        return 'Ruim', 2
    
    if 'vago' in problemas:
        return 'Regular', 3
    
    if len(problemas) > 0:
        return 'Regular', 4
    
    if len(texto) < 100:
        return 'Bom', 7
    
    if len(texto) < 200:
        return 'Bom', 8
    
    return 'Excelente', 10

# Fun√ß√£o para analisar consist√™ncia do sucesso de contato
def verificar_indicacao(texto, palavras_lista, palavras_excecao=None):
    if pd.isna(texto) or not isinstance(texto, str):
        return False
    
    texto_lower = texto.lower()
    
    # Verificar exce√ß√µes primeiro
    if palavras_excecao:
        for excecao in palavras_excecao:
            if excecao in texto_lower:
                return False
    
    for palavra in palavras_lista:
        if palavra in texto_lower:
            return True
    return False

# Fun√ß√£o para classificar a consist√™ncia entre o campo de sucesso e a descri√ß√£o
def classificar_consistencia(row):
    if pd.isna(row['acompanhamento_sucesso_contato']):
        return 'Indefinido'
    
    # Casos onde h√° indica√ß√µes contradit√≥rias na descri√ß√£o
    if row['indicacao_sucesso_na_descricao'] and row['indicacao_insucesso_na_descricao']:
        # Verificar contexto adicional
        if row['contexto_objetivo_nao_atingido']:
            # Se h√° indica√ß√£o de que o objetivo n√£o foi atingido, mas o contato foi feito
            if row['acompanhamento_sucesso_contato'] == 'Sim':
                return 'Consistente (Sucesso)'
            else:
                return 'Poss√≠vel Inconsist√™ncia (N√£o/Sucesso)'
        else:
            return 'Indica√ß√µes contradit√≥rias na descri√ß√£o'
    
    # Casos onde a coluna indica sucesso ("Sim") e a descri√ß√£o tamb√©m indica sucesso
    if row['acompanhamento_sucesso_contato'] == 'Sim' and row['indicacao_sucesso_na_descricao'] and not row['indicacao_insucesso_na_descricao']:
        return 'Consistente (Sucesso)'
    
    # Casos onde a coluna indica insucesso ("N√£o") e a descri√ß√£o tamb√©m indica insucesso
    if row['acompanhamento_sucesso_contato'] == 'N√£o' and row['indicacao_insucesso_na_descricao'] and not row['indicacao_sucesso_na_descricao']:
        return 'Consistente (Insucesso)'
    
    # Casos onde o contato foi feito, mas o objetivo n√£o foi atingido
    if row['acompanhamento_sucesso_contato'] == 'Sim' and row['indicacao_insucesso_na_descricao'] and row['contexto_objetivo_nao_atingido']:
        return 'Consistente (Sucesso)'
    
    # Casos de poss√≠vel inconsist√™ncia - coluna diz "Sim" mas descri√ß√£o sugere insucesso no contato
    if row['acompanhamento_sucesso_contato'] == 'Sim' and row['indicacao_insucesso_na_descricao'] and not row['contexto_objetivo_nao_atingido']:
        return 'Poss√≠vel Inconsist√™ncia (Sim/Insucesso)'
    
    # Casos de poss√≠vel inconsist√™ncia - coluna diz "N√£o" mas descri√ß√£o sugere sucesso
    if row['acompanhamento_sucesso_contato'] == 'N√£o' and row['indicacao_sucesso_na_descricao']:
        return 'Poss√≠vel Inconsist√™ncia (N√£o/Sucesso)'
    
    # Casos sem clara indica√ß√£o na descri√ß√£o
    return 'Sem indica√ß√£o clara na descri√ß√£o'

# Fun√ß√£o para carregar os dados
@st.cache_data
def carregar_dados(uploaded_file=None):
    # Verificar se um arquivo foi carregado pelo usu√°rio
    if uploaded_file is not None:
        # Carregar dados do arquivo enviado pelo usu√°rio
        df = pd.read_excel(uploaded_file)
    else:
        try:
            # Tentar carregar dados do arquivo local
            arquivo_padrao = 'TR_Verif_Acomp_Cariacica.xlsx'
            if os.path.exists(arquivo_padrao):
                df = pd.read_excel(arquivo_padrao)
            else:
                # Se n√£o encontrar o arquivo, retornar None
                st.error(f"Arquivo {arquivo_padrao} n√£o encontrado. Por favor, fa√ßa o upload do arquivo.")
                return None
        except Exception as e:
            st.error(f"Erro ao carregar dados: {str(e)}")
            return None
    
    # Garantir que as colunas de interesse existam
    colunas_interesse = [
        'acompanhamento_descricao', 
        'acompanhamento_articulador', 
        'acompanhamento_data', 
        'acompanhamento_sucesso_contato',
        'dado_algum_encaminhamento',
        'instituicao_encaminhamento_educacao',
        'instituicao_encaminhamento_saude',
        'instituicao_encaminhamento_assistencia_social',
        'instituicao_encaminhamento_conselho_tutelar',
        'instituicao_encaminhamento_estacao_conhecimento',
        'instituicao_encaminhamento_sociedade_civil',
        'instituicao_encaminhamento_outro_equipamento'
    ]
    
    # Verificar colunas obrigat√≥rias
    colunas_obrigatorias = ['acompanhamento_descricao', 'acompanhamento_articulador', 'acompanhamento_data', 'acompanhamento_sucesso_contato']
    for coluna in colunas_obrigatorias:
        if coluna not in df.columns:
            st.error(f"Coluna '{coluna}' n√£o encontrada no arquivo!")
            st.stop()
    
    # Verificar colunas de encaminhamento e criar se n√£o existirem (para evitar erros)
    colunas_encaminhamento = [
        'dado_algum_encaminhamento',
        'instituicao_encaminhamento_educacao',
        'instituicao_encaminhamento_saude',
        'instituicao_encaminhamento_assistencia_social',
        'instituicao_encaminhamento_conselho_tutelar',
        'instituicao_encaminhamento_estacao_conhecimento',
        'instituicao_encaminhamento_sociedade_civil',
        'instituicao_encaminhamento_outro_equipamento'
    ]
    
    for coluna in colunas_encaminhamento:
        if coluna not in df.columns:
            st.warning(f"Coluna '{coluna}' n√£o encontrada! Ser√° criada uma coluna vazia.")
            df[coluna] = None
    
    # Adicionar coluna de texto pr√©-processado
    df['texto_preprocessado'] = df['acompanhamento_descricao'].apply(preprocessar_texto)
    
    # Identificar problemas nas descri√ß√µes
    df['problemas_detectados'] = df['acompanhamento_descricao'].apply(detectar_problemas)
    
    # Quantificar o n√∫mero de problemas
    df['num_problemas'] = df['problemas_detectados'].apply(len)
    
    # Classificar a qualidade dos registros
    df['classificacao_qualidade'], df['pontuacao'] = zip(*df['acompanhamento_descricao'].apply(
        lambda x: classificar_qualidade(x)))
    
    # Quantificar o comprimento das descri√ß√µes
    df['comprimento_descricao'] = df['acompanhamento_descricao'].apply(
        lambda x: len(x) if isinstance(x, str) else 0)
    
    # Garantir que a data est√° no formato correto
    df['acompanhamento_data'] = pd.to_datetime(df['acompanhamento_data'], errors='coerce')
    
    # Filtrar datas futuras ou claramente incorretas (como novembro 2025)
    data_atual = datetime.now()
    df = df[df['acompanhamento_data'] <= data_atual]
    
    # Adicionar aviso se foram removidas datas futuras
    if (df['acompanhamento_data'] > data_atual).any():
        st.warning(f"Foram removidos registros com datas futuras.")
    
    # Adicionar colunas para an√°lise temporal
    df['ano'] = df['acompanhamento_data'].dt.year
    df['mes_num'] = df['acompanhamento_data'].dt.month  # Para ordena√ß√£o
    df['mes'] = df['acompanhamento_data'].dt.month_name()
    # Formatar semana como 'YYYY-MM-WW' para garantir ordena√ß√£o correta
    df['semana'] = df['acompanhamento_data'].dt.strftime('%Y-%m-W%U')
    df['dia_semana'] = df['acompanhamento_data'].dt.day_name()
    # Formatar mes_ano para garantir ordena√ß√£o correta: 'YYYY-MM-MesNome'
    df['mes_ano'] = df['acompanhamento_data'].dt.strftime('%Y-%m-%b')
    df['mes_ano_display'] = df['acompanhamento_data'].dt.strftime('%b %Y')
    
    # Adicionar an√°lise de consist√™ncia entre sucesso de contato e descri√ß√£o
    
    # Palavras e express√µes que indicam sucesso de contato
    palavras_sucesso = [
        'com sucesso', 'realizado com sucesso', 'atendido', 'conseguiu', 'conseguimos',
        'contato realizado', 'efetuado com sucesso', 'foi atendido', 'foi realizado',
        'estabelecido contato', 'respondeu', 'responderam', 'atendeu', 'atenderam',
        'em contato com', 'de acordo com', 'segundo', 'conforme', 'informou que',
        'relatou que', 'disse que', 'comunicado que', 'contato feito', 'foi comunicado',
        'entramos em contato', 'entramos em contato com', 'recebi contato', 'recebi retorno',
        'recebemos contato', 'recebemos retorno', 'informou-se', 'foi informado'
    ]
    
    # Palavras e express√µes que indicam insucesso no contato
    palavras_insucesso = [
        'n√£o atendeu', 'n√£o respondeu', 'sem sucesso', 'fracassou',
        'ningu√©m atendeu', 'n√£o conseguimos contato', 'n√£o obtive retorno',
        'n√£o obtivemos retorno', 'n√£o conseguimos contatar', 'n√£o atende',
        'contato sem sucesso', 'tentativa sem sucesso', 'n√£o foi atendido',
        'n√£o foi poss√≠vel contatar', 'n√£o foi localizado', 'caixa postal',
        'chamou at√© cair', 'telefone desligado', 'fora de √°rea', 'tentei contato',
        'n√∫mero inexistente', 'desligou a liga√ß√£o', 'n√£o foi poss√≠vel falar',
        'tentamos contato'
    ]
    
    # Palavras que indicam que o objetivo n√£o foi atingido (mesmo que o contato tenha acontecido)
    palavras_objetivo_nao_atingido = [
        'n√£o foi poss√≠vel obter', 'n√£o foi poss√≠vel localizar', 'n√£o foi encontrado',
        'n√£o consta', 'n√£o possui registro', 'n√£o possui cadastro', 'n√£o h√° registro',
        'n√£o h√° cadastro', 'sem registro', 'sem cadastro', 'n√£o foi poss√≠vel verificar',
        'n√£o identificamos', 'n√£o identificado', 'n√£o localizado', 'n√£o localizamos',
        'n√£o foi identificado', 'n√£o obteve √™xito', 'mas n√£o conseguiu', 'mas n√£o conseguimos',
        'n√£o dispon√≠vel', 'n√£o disp√µe', 'n√£o foi disponibilizado', 'negou', 'recusou',
        'recusou-se', 'n√£o informou', 'n√£o quis informar', 'n√£o tem conhecimento',
        'desconhece', 'n√£o tem informa√ß√µes', 'n√£o soube dizer', 'n√£o soube informar',
        'n√£o obteve', 'n√£o recebemos', 'n√£o recebeu', 'n√£o tinha', 'n√£o possui',
        'n√£o foram encontrados', 'n√£o foram localizados', 'n√£o apresentou',
        'n√£o apresentaram', 'n√£o trouxe', 'n√£o trouxeram'
    ]
    
    # Express√µes espec√≠ficas para verificar se o contato real foi bem sucedido
    indicadores_contato_real = [
        'de acordo com', 'segundo', 'informou que', 'relatou que', 'disse que',
        'foi informado', 'foi comunicado', 'foi relatado', 'comunicou que',
        'em conversa com', 'conversei com', 'em contato com', 'contato com',
        'em reuni√£o com', 'durante reuni√£o', 'durante encontro', 'segundo informa√ß√µes',
        'conforme relatado', 'conforme dito', 'conforme informado', 'em atendimento',
        'durante atendimento', 'ap√≥s conversa', 'em di√°logo', 'visitamos', 'visitei'
    ]
    
    # Adicionar colunas de indica√ß√£o baseadas na descri√ß√£o
    # Verificar se h√° indica√ß√£o de sucesso no contato
    df['indicacao_sucesso_na_descricao'] = df['acompanhamento_descricao'].apply(
        lambda x: verificar_indicacao(x, palavras_sucesso))
        
    # Verificar se h√° indica√ß√£o de insucesso no contato
    df['indicacao_insucesso_na_descricao'] = df['acompanhamento_descricao'].apply(
        lambda x: verificar_indicacao(x, palavras_insucesso))
    
    # Verificar se o contato ocorreu mas o objetivo n√£o foi atingido
    df['contexto_objetivo_nao_atingido'] = df.apply(
        lambda row: (any(termo in row['acompanhamento_descricao'].lower() for termo in indicadores_contato_real) and
                     any(termo in row['acompanhamento_descricao'].lower() for termo in palavras_objetivo_nao_atingido))
        if isinstance(row['acompanhamento_descricao'], str) else False, axis=1)
    
    # Classificar a consist√™ncia
    df['consistencia_sucesso'] = df.apply(classificar_consistencia, axis=1)
    
    # Processar dados de encaminhamentos
    
    # Garantir que a coluna 'dado_algum_encaminhamento' tenha valores preenchidos
    if df['dado_algum_encaminhamento'].isna().any():
        # Preencher valores nulos com 'N√£o'
        df['dado_algum_encaminhamento'] = df['dado_algum_encaminhamento'].fillna('N√£o')
    
    # Verificar consist√™ncia de encaminhamentos (Sim deve ter pelo menos uma institui√ß√£o preenchida)
    colunas_instituicoes = [
        'instituicao_encaminhamento_educacao',
        'instituicao_encaminhamento_saude',
        'instituicao_encaminhamento_assistencia_social',
        'instituicao_encaminhamento_conselho_tutelar',
        'instituicao_encaminhamento_estacao_conhecimento',
        'instituicao_encaminhamento_sociedade_civil',
        'instituicao_encaminhamento_outro_equipamento'
    ]
    
    # Adicionar coluna com o n√∫mero de institui√ß√µes encaminhadas por registro
    df['num_instituicoes_encaminhadas'] = df[colunas_instituicoes].notna().sum(axis=1)
    
    # Identificar inconsist√™ncias (Sim sem institui√ß√µes ou N√£o com institui√ß√µes)
    df['inconsistencia_encaminhamento'] = (
        ((df['dado_algum_encaminhamento'] == 'Sim') & (df['num_instituicoes_encaminhadas'] == 0)) |
        ((df['dado_algum_encaminhamento'] == 'N√£o') & (df['num_instituicoes_encaminhadas'] > 0))
    )
    
    return df

# Fun√ß√£o para calcular similaridade entre descri√ß√µes de acompanhamentos
def calcular_similaridade_descricoes(df):
    """
    Calcula a similaridade entre as descri√ß√µes dos acompanhamentos
    usando TF-IDF e similaridade de coseno.
    Considera apenas registros do mesmo ID como poss√≠veis duplica√ß√µes.
    """
    try:
        # Mostrar status para o usu√°rio
        placeholder = st.empty()
        placeholder.info("Iniciando an√°lise de similaridade...")
        
        # Adicionando um contador para registros processados
        contador_progresso = 0
        total_registros = len(df)
        
        # Filtrar apenas registros com descri√ß√£o v√°lida
        placeholder.info("Filtrando registros com descri√ß√µes v√°lidas...")
        df_validos = df[df['acompanhamento_descricao'].notna() & 
                        (df['acompanhamento_descricao'].str.len() > 10)]
        
        if len(df_validos) <= 1:
            placeholder.info("N√£o h√° registros suficientes para an√°lise de similaridade.")
            return pd.DataFrame()  # Retorna DataFrame vazio se n√£o houver registros suficientes
        
        # Verificar se existe a coluna 'id'
        if 'id' not in df_validos.columns:
            placeholder.warning("Coluna 'id' n√£o encontrada. A verifica√ß√£o de duplica√ß√µes ser√° feita sem considerar o ID da crian√ßa.")
            possui_id = False
        else:
            possui_id = True
        
        # Limitar o n√∫mero de registros para processamento se for muito grande
        max_registros = 1000  # Definir um limite razo√°vel
        if len(df_validos) > max_registros:
            placeholder.warning(f"Muitos registros encontrados ({len(df_validos)}). Limitando a an√°lise aos {max_registros} primeiros para evitar lentid√£o.")
            df_validos = df_validos.head(max_registros)
        
        # Preparar o vetorizador TF-IDF
        placeholder.info(f"Preparando vetoriza√ß√£o TF-IDF para {len(df_validos)} registros...")
        tfidf_vectorizer = TfidfVectorizer(
            min_df=1, 
            stop_words=stopwords.words('portuguese'),
            lowercase=True,
            strip_accents='unicode',
            max_features=5000  # Limitar o n√∫mero de features para melhorar performance
        )
        
        # Criar a matriz TF-IDF
        placeholder.info("Vetorizando textos...")
        tfidf_matrix = tfidf_vectorizer.fit_transform(df_validos['acompanhamento_descricao'])
        
        # Calcular a similaridade de coseno
        placeholder.info("Calculando matriz de similaridade (pode levar alguns segundos)...")
        # Usar blocos menores para c√°lculo de similaridade para grandes conjuntos de dados
        batch_size = 100
        resultados = []
        
        # Se o conjunto de dados for grande, processar em lotes
        if len(df_validos) > batch_size:
            total_batches = (len(df_validos) // batch_size) + 1
            current_batch = 0
            
            for i in range(0, len(df_validos), batch_size):
                current_batch += 1
                end = min(i + batch_size, len(df_validos))
                placeholder.info(f"Processando lote {current_batch}/{total_batches} de compara√ß√µes...")
                
                # Calcular similaridade para este lote
                batch_matrix = tfidf_matrix[i:end]
                cosine_sim_batch = cosine_similarity(batch_matrix, tfidf_matrix)
                
                # Processar resultados deste lote
                for batch_idx, global_i in enumerate(range(i, end)):
                    for global_j in range(global_i + 1, len(df_validos)):
                        # Verificar se s√£o do mesmo ID (se a coluna existir)
                        if possui_id and df_validos.iloc[global_i]['id'] != df_validos.iloc[global_j]['id']:
                            continue  # Pular se n√£o for o mesmo ID
                        
                        # Obter o valor de similaridade
                        similarity = cosine_sim_batch[batch_idx, global_j]
                        
                        if similarity >= 0.7:  # Threshold de similaridade (70%)
                            resultados.append(_criar_registro_similaridade(
                                df_validos, global_i, global_j, similarity, possui_id
                            ))
                            
                            # Atualizar contador
                            contador_progresso += 1
                            if contador_progresso % 10 == 0:
                                placeholder.info(f"Encontradas {contador_progresso} duplica√ß√µes potenciais at√© agora...")
        else:
            # Para conjuntos menores, calcular a matriz de similaridade completa
            cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
            
            # Iterar sobre a matriz de similaridade
            placeholder.info("Analisando resultados de similaridade...")
            for i in range(len(cosine_sim)):
                for j in range(i+1, len(cosine_sim)):  # Come√ßar do pr√≥ximo item para evitar auto-compara√ß√£o
                    # Verificar se s√£o do mesmo ID (se a coluna existir)
                    if possui_id:
                        mesmo_id = df_validos.iloc[i]['id'] == df_validos.iloc[j]['id']
                        if not mesmo_id:
                            continue  # Pular se n√£o for o mesmo ID
                    
                    if cosine_sim[i][j] >= 0.7:  # Threshold de similaridade (70%)
                        resultados.append(_criar_registro_similaridade(
                            df_validos, i, j, cosine_sim[i][j], possui_id
                        ))
                        
                        # Atualizar contador
                        contador_progresso += 1
                        if contador_progresso % 10 == 0:
                            placeholder.info(f"Encontradas {contador_progresso} duplica√ß√µes potenciais at√© agora...")
        
        # Criar DataFrame com os resultados
        placeholder.info("Finalizando an√°lise...")
        df_similaridade = pd.DataFrame(resultados)
        
        # Ordenar por similaridade (decrescente)
        if not df_similaridade.empty:
            df_similaridade = df_similaridade.sort_values('similaridade', ascending=False)
            placeholder.success(f"An√°lise conclu√≠da com sucesso! Encontradas {len(df_similaridade)} poss√≠veis duplica√ß√µes.")
        else:
            placeholder.info("An√°lise conclu√≠da, mas nenhuma duplica√ß√£o foi encontrada.")
        
        # Limpar mensagem de status ap√≥s concluir
        placeholder.empty()
        
        return df_similaridade
    
    except Exception as e:
        st.error(f"Erro ao calcular similaridade: {str(e)}")
        import traceback
        st.error(f"Detalhes: {traceback.format_exc()}")
        return pd.DataFrame()

def _criar_registro_similaridade(df_validos, i, j, similaridade, possui_id):
    """Fun√ß√£o auxiliar para criar um registro de similaridade"""
    linha_i = df_validos.iloc[i]
    linha_j = df_validos.iloc[j]
    
    # Verificar se s√£o do mesmo cadastro (mesmo aluno_nome se dispon√≠vel)
    mesmo_beneficiario = "N√£o verificado"
    if 'aluno_nome' in df_validos.columns:
        mesmo_beneficiario = "Sim" if linha_i['aluno_nome'] == linha_j['aluno_nome'] else "N√£o"
    
    # Verificar se as datas s√£o as mesmas ou pr√≥ximas
    data_i = linha_i['acompanhamento_data']
    data_j = linha_j['acompanhamento_data']
    diff_dias = abs((data_i - data_j).days)
    
    # Adicionar o ID aos resultados
    id_registro = linha_i['id'] if possui_id else "N√£o dispon√≠vel"
    
    # Retornar um dicion√°rio com todos os dados relevantes
    return {
        'id_registro_1': df_validos.index[i],
        'id_registro_2': df_validos.index[j],
        'acompanhamento_cod_1': linha_i['acompanhamento_cod'] if 'acompanhamento_cod' in linha_i else '',
        'acompanhamento_cod_2': linha_j['acompanhamento_cod'] if 'acompanhamento_cod' in linha_j else '',
        'id_crianca': id_registro,
        'similaridade': similaridade,
        'data_registro_1': data_i,
        'data_registro_2': data_j,
        'diferenca_dias': diff_dias,
        'articulador_1': linha_i['acompanhamento_articulador'],
        'articulador_2': linha_j['acompanhamento_articulador'],
        'mesmo_beneficiario': mesmo_beneficiario,
        'descricao_1': linha_i['acompanhamento_descricao'],
        'descricao_2': linha_j['acompanhamento_descricao']
    }

def detectar_encaminhamentos_duplicados(df, dias_limite=30):
    """
    Detecta poss√≠veis encaminhamentos duplicados para o mesmo ID (crian√ßa) em um per√≠odo pr√≥ximo.
    
    Args:
        df: DataFrame com os registros de encaminhamentos
        dias_limite: N√∫mero m√°ximo de dias para considerar como per√≠odo pr√≥ximo (padr√£o: 30 dias)
        
    Returns:
        DataFrame com os poss√≠veis encaminhamentos duplicados
    """
    # Verificar se existe a coluna 'id'
    if 'id' not in df.columns:
        st.warning("Coluna 'id' n√£o encontrada. A verifica√ß√£o de encaminhamentos duplicados n√£o pode ser realizada.")
        return pd.DataFrame()
    
    # Filtrar apenas registros com encaminhamentos
    df_encaminhamentos = df[df['dado_algum_encaminhamento'] == 'Sim'].copy()
    
    if len(df_encaminhamentos) <= 1:
        return pd.DataFrame()  # Retorna DataFrame vazio se n√£o houver registros suficientes
    
    # Inicializar lista para armazenar duplica√ß√µes potenciais
    duplicacoes = []
    
    # Lista de colunas de institui√ß√µes
    colunas_instituicoes = [
        'instituicao_encaminhamento_educacao',
        'instituicao_encaminhamento_saude',
        'instituicao_encaminhamento_assistencia_social',
        'instituicao_encaminhamento_conselho_tutelar',
        'instituicao_encaminhamento_estacao_conhecimento',
        'instituicao_encaminhamento_sociedade_civil',
        'instituicao_encaminhamento_outro_equipamento'
    ]
    
    # Agrupar por ID
    grupos_id = df_encaminhamentos.groupby('id')
    
    # Para cada ID, verificar se h√° encaminhamentos pr√≥ximos
    for id_crianca, grupo in grupos_id:
        # Se houver apenas um registro para este ID, n√£o h√° duplica√ß√µes
        if len(grupo) <= 1:
            continue
        
        # Ordenar por data
        grupo_ordenado = grupo.sort_values('acompanhamento_data')
        
        # Comparar cada par de registros
        for i in range(len(grupo_ordenado)):
            for j in range(i+1, len(grupo_ordenado)):
                linha_i = grupo_ordenado.iloc[i]
                linha_j = grupo_ordenado.iloc[j]
                
                # Calcular diferen√ßa de dias
                data_i = linha_i['acompanhamento_data']
                data_j = linha_j['acompanhamento_data']
                diff_dias = abs((data_j - data_i).days)
                
                # Se a diferen√ßa for maior que o limite, n√£o considerar como duplica√ß√£o
                if diff_dias > dias_limite:
                    continue
                
                # Verificar se h√° pelo menos uma institui√ß√£o em comum
                tem_instituicao_comum = False
                instituicoes_comuns = []
                
                for coluna in colunas_instituicoes:
                    # Verificar se ambos os registros t√™m valor n√£o-nulo para esta institui√ß√£o
                    if pd.notna(linha_i[coluna]) and pd.notna(linha_j[coluna]):
                        tem_instituicao_comum = True
                        instituicoes_comuns.append(coluna.replace('instituicao_encaminhamento_', ''))
                
                # Se houver pelo menos uma institui√ß√£o em comum, considerar como poss√≠vel duplica√ß√£o
                if tem_instituicao_comum:
                    duplicacoes.append({
                        'id_crianca': id_crianca,
                        'id_registro_1': grupo_ordenado.index[i],
                        'id_registro_2': grupo_ordenado.index[j],
                        'acompanhamento_cod_1': linha_i['acompanhamento_cod'] if 'acompanhamento_cod' in linha_i else '',
                        'acompanhamento_cod_2': linha_j['acompanhamento_cod'] if 'acompanhamento_cod' in linha_j else '',
                        'data_registro_1': data_i,
                        'data_registro_2': data_j,
                        'diferenca_dias': diff_dias,
                        'articulador_1': linha_i['acompanhamento_articulador'],
                        'articulador_2': linha_j['acompanhamento_articulador'],
                        'mesmo_articulador': linha_i['acompanhamento_articulador'] == linha_j['acompanhamento_articulador'],
                        'instituicoes_comuns': ', '.join(instituicoes_comuns),
                        'descricao_1': linha_i['acompanhamento_descricao'],
                        'descricao_2': linha_j['acompanhamento_descricao']
                    })
    
    # Criar DataFrame com os resultados
    df_duplicacoes = pd.DataFrame(duplicacoes)
    
    # Ordenar por diferen√ßa de dias (crescente) e depois por ID
    if not df_duplicacoes.empty:
        df_duplicacoes = df_duplicacoes.sort_values(['diferenca_dias', 'id_crianca'])
    
    return df_duplicacoes

# Fun√ß√£o para detectar encaminhamentos sem seguimento (follow-up)
def detectar_encaminhamentos_sem_followup(df, dias_limite=30):
    """
    Detecta encaminhamentos que n√£o tiveram follow-up (seguimento) em registros posteriores.
    
    Args:
        df: DataFrame com os registros
        dias_limite: N√∫mero de dias que devem passar para considerar que deveria haver um follow-up
        
    Returns:
        DataFrame com os encaminhamentos sem follow-up detectados
    """
    # Verificar se existe a coluna 'id'
    if 'id' not in df.columns:
        st.warning("Coluna 'id' n√£o encontrada. A verifica√ß√£o de encaminhamentos sem follow-up n√£o pode ser realizada.")
        return pd.DataFrame()
    
    # Garantir que a data est√° no formato correto
    if not pd.api.types.is_datetime64_any_dtype(df['acompanhamento_data']):
        df['acompanhamento_data'] = pd.to_datetime(df['acompanhamento_data'], errors='coerce')
    
    # Filtrar apenas registros com encaminhamentos
    df_encaminhamentos = df[df['dado_algum_encaminhamento'] == 'Sim'].copy()
    
    if len(df_encaminhamentos) <= 0:
        return pd.DataFrame()  # Retorna DataFrame vazio se n√£o houver registros suficientes
    
    # Lista de colunas de institui√ß√µes
    colunas_instituicoes = [
        'instituicao_encaminhamento_educacao',
        'instituicao_encaminhamento_saude',
        'instituicao_encaminhamento_assistencia_social',
        'instituicao_encaminhamento_conselho_tutelar',
        'instituicao_encaminhamento_estacao_conhecimento',
        'instituicao_encaminhamento_sociedade_civil',
        'instituicao_encaminhamento_outro_equipamento'
    ]
    
    # Data de refer√™ncia (data atual)
    data_referencia = datetime.now()
    
    # Lista para armazenar encaminhamentos sem follow-up
    encaminhamentos_sem_followup = []
    
    # Agrupar por ID
    grupos_id = df.groupby('id')
    
    # Para cada ID, verificar encaminhamentos sem follow-up
    for id_crianca, grupo in grupos_id:
        # Ordenar por data para cada crian√ßa
        grupo_ordenado = grupo.sort_values('acompanhamento_data')
        
        # Obter apenas os registros com encaminhamentos para esta crian√ßa
        encaminhamentos_crianca = grupo_ordenado[grupo_ordenado['dado_algum_encaminhamento'] == 'Sim']
        
        if encaminhamentos_crianca.empty:
            continue
        
        # Analisar cada encaminhamento
        for idx, encaminhamento in encaminhamentos_crianca.iterrows():
            data_encaminhamento = encaminhamento['acompanhamento_data']
            
            # Verificar se j√° passaram os dias limite desde o encaminhamento
            if (data_referencia - data_encaminhamento).days < dias_limite:
                continue  # Ainda n√£o passou tempo suficiente para considerar sem follow-up
            
            # Listar institui√ß√µes encaminhadas neste registro
            instituicoes_encaminhadas = []
            for coluna in colunas_instituicoes:
                if pd.notna(encaminhamento[coluna]):
                    tipo_instituicao = coluna.replace('instituicao_encaminhamento_', '')
                    nome_instituicao = encaminhamento[coluna]
                    instituicoes_encaminhadas.append((tipo_instituicao, nome_instituicao))
            
            if not instituicoes_encaminhadas:
                continue  # N√£o h√° institui√ß√µes espec√≠ficas neste encaminhamento
            
            # Verificar registros posteriores
            registros_posteriores = grupo_ordenado[grupo_ordenado['acompanhamento_data'] > data_encaminhamento]
            
            # Se n√£o h√° registros posteriores, √© um encaminhamento sem follow-up
            if registros_posteriores.empty:
                foi_seguido = False
            else:
                # Verificar se h√° alguma men√ß√£o de follow-up nos registros posteriores
                foi_seguido = False
                
                # Verificar na descri√ß√£o dos registros posteriores
                for _, reg_posterior in registros_posteriores.iterrows():
                    descricao = str(reg_posterior['acompanhamento_descricao']).lower() if pd.notna(reg_posterior['acompanhamento_descricao']) else ""
                    
                    # Verificar para cada institui√ß√£o se h√° men√ß√£o na descri√ß√£o
                    for tipo_inst, nome_inst in instituicoes_encaminhadas:
                        if pd.notna(nome_inst) and str(nome_inst).lower() in descricao:
                            foi_seguido = True
                            break
                    
                    # Verificar se o tipo de sinaliza√ß√£o √© relacionado a follow-up
                    tipo_sinalizacao = reg_posterior.get('acompanhamento_tipo_sinalizacao', '')
                    if pd.notna(tipo_sinalizacao) and tipo_sinalizacao in ['frequencias', 'acoes_complementares']:
                        foi_seguido = True
                        break
                    
                    if foi_seguido:
                        break
            
            # Se n√£o h√° seguimento, adicionar √† lista
            if not foi_seguido:
                encaminhamentos_sem_followup.append({
                    'id_crianca': id_crianca,
                    'id_registro': idx,
                    'acompanhamento_cod': encaminhamento['acompanhamento_cod'] if 'acompanhamento_cod' in encaminhamento else '',
                    'data_encaminhamento': data_encaminhamento,
                    'dias_desde_encaminhamento': (data_referencia - data_encaminhamento).days,
                    'articulador': encaminhamento['acompanhamento_articulador'],
                    'instituicoes_encaminhadas': ', '.join([f"{tipo} - {nome}" for tipo, nome in instituicoes_encaminhadas]),
                    'descricao': encaminhamento['acompanhamento_descricao']
                })
    
    # Criar DataFrame com os resultados
    df_sem_followup = pd.DataFrame(encaminhamentos_sem_followup)
    
    # Ordenar por n√∫mero de dias (decrescente)
    if not df_sem_followup.empty:
        df_sem_followup = df_sem_followup.sort_values('dias_desde_encaminhamento', ascending=False)
    
    return df_sem_followup

# MAIN APP
st.title('TR | COMEA - üìä An√°lise de Qualidade dos Registros de Acompanhamento')

# Barra lateral para sele√ß√£o de abas
tab_selecionada = st.sidebar.radio(
    "Selecione uma se√ß√£o:",
    ["Vis√£o Geral", 
     "An√°lise Temporal", 
     "An√°lise por Articulador", 
     "Exemplos de Problemas", 
     "An√°lise de Texto",
     "Consist√™ncia de Sucesso",
     "An√°lise de Encaminhamentos",
     "Monitoramento de Follow-up",
     "Detec√ß√£o de Duplicados"]
)

# √Årea para upload de arquivo
st.sidebar.header('Upload de Dados')
uploaded_file = st.sidebar.file_uploader("Fa√ßa upload do arquivo Excel", type=['xlsx'])

# Carregar dados
with st.spinner('Carregando e processando dados...'):
    df = carregar_dados(uploaded_file)

# Verificar se os dados foram carregados
if df is None:
    st.warning("Nenhum arquivo carregado. Por favor, fa√ßa o upload do arquivo Excel com os dados de acompanhamento.")
    st.stop()  # Parar a execu√ß√£o do aplicativo at√© que o arquivo seja carregado

# Sidebar para filtros
st.sidebar.header('Filtros')

# Bot√£o para limpar filtros
if st.sidebar.button('Limpar Todos os Filtros'):
    # Atualiza√ß√£o: usando as fun√ß√µes recomendadas em vez das experimentais
    st.query_params.clear()
    st.rerun()

# Filtro por per√≠odo
st.sidebar.subheader('Per√≠odo de An√°lise')
data_min = df['acompanhamento_data'].min().date()
data_max = df['acompanhamento_data'].max().date()

# Definir data inicial padr√£o como 01/01/2025 (ou a data m√≠nima se for maior)
data_inicio_padrao = max(datetime(2025, 1, 1).date(), data_min)

# Criar filtro de per√≠odo (data inicial e final)
col1, col2 = st.sidebar.columns(2)
with col1:
    data_inicio = st.date_input('Data Inicial', data_inicio_padrao, min_value=data_min, max_value=data_max)
with col2:
    data_fim = st.date_input('Data Final', data_max, min_value=data_min, max_value=data_max)

# Filtro por articulador
articuladores = ['Todos'] + sorted(df['acompanhamento_articulador'].unique().tolist())
articulador_selecionado = st.sidebar.selectbox('Articulador', articuladores)

# Filtro por qualidade
qualidades = ['Todos', 'Excelente', 'Bom', 'Regular', 'Ruim', 'Cr√≠tico']
qualidade_selecionada = st.sidebar.selectbox('Classifica√ß√£o de Qualidade', qualidades)

# Filtro por tipo de problema
tipos_problemas = ['Todos', 'muito_curto', 'vago', 'sem_detalhes', 'generico', 'texto_nulo', 'texto_invalido']
problema_selecionado = st.sidebar.selectbox('Tipo de Problema', tipos_problemas)

# Filtro por sucesso de contato e consist√™ncia
st.sidebar.subheader('Filtros Adicionais')
sucessos_contato = ['Todos', 'Sim', 'N√£o']
sucesso_selecionado = st.sidebar.selectbox('Sucesso de Contato', sucessos_contato)

tipos_consistencia = ['Todos', 'Consistente (Sucesso)', 'Consistente (Insucesso)', 
                     'Poss√≠vel Inconsist√™ncia (Sim/Insucesso)', 'Poss√≠vel Inconsist√™ncia (N√£o/Sucesso)',
                     'Sem indica√ß√£o clara na descri√ß√£o', 'Indica√ß√µes contradit√≥rias na descri√ß√£o']
consistencia_selecionada = st.sidebar.selectbox('Consist√™ncia de Sucesso', tipos_consistencia)

# Filtro de contexto de objetivo
contextos = ['Todos', 'Objetivo N√£o Atingido', 'Sem Indica√ß√£o de Objetivo']
contexto_selecionado = st.sidebar.selectbox('Contexto do Registro', contextos)

# Filtros para encaminhamentos
st.sidebar.subheader('Filtros de Encaminhamentos')
encaminhamentos_filtro = ['Todos', 'Com Encaminhamento', 'Sem Encaminhamento']
encaminhamento_selecionado = st.sidebar.selectbox('Status de Encaminhamento', encaminhamentos_filtro)

# Filtro por tipo de institui√ß√£o encaminhada
tipos_instituicoes = [
    'Todos',
    'Educa√ß√£o',
    'Sa√∫de',
    'Assist√™ncia Social',
    'Conselho Tutelar',
    'Esta√ß√£o Conhecimento',
    'Sociedade Civil',
    'Outro Equipamento'
]
instituicao_selecionada = st.sidebar.selectbox('Tipo de Institui√ß√£o Encaminhada', tipos_instituicoes)

# Aplicar filtros
df_filtrado = df.copy()

# Filtro de per√≠odo
df_filtrado = df_filtrado[(df_filtrado['acompanhamento_data'].dt.date >= data_inicio) & 
                           (df_filtrado['acompanhamento_data'].dt.date <= data_fim)]

if articulador_selecionado != 'Todos':
    df_filtrado = df_filtrado[df_filtrado['acompanhamento_articulador'] == articulador_selecionado]

if qualidade_selecionada != 'Todos':
    df_filtrado = df_filtrado[df_filtrado['classificacao_qualidade'] == qualidade_selecionada]

if problema_selecionado != 'Todos':
    df_filtrado = df_filtrado[df_filtrado['problemas_detectados'].apply(lambda x: problema_selecionado in x)]

if sucesso_selecionado != 'Todos':
    df_filtrado = df_filtrado[df_filtrado['acompanhamento_sucesso_contato'] == sucesso_selecionado]

if consistencia_selecionada != 'Todos':
    df_filtrado = df_filtrado[df_filtrado['consistencia_sucesso'] == consistencia_selecionada]

if contexto_selecionado != 'Todos':
    if contexto_selecionado == 'Objetivo N√£o Atingido':
        df_filtrado = df_filtrado[df_filtrado['contexto_objetivo_nao_atingido'] == True]
    elif contexto_selecionado == 'Sem Indica√ß√£o de Objetivo':
        df_filtrado = df_filtrado[df_filtrado['contexto_objetivo_nao_atingido'] == False]

# Aplicar filtros de encaminhamento
if encaminhamento_selecionado != 'Todos':
    if encaminhamento_selecionado == 'Com Encaminhamento':
        df_filtrado = df_filtrado[df_filtrado['dado_algum_encaminhamento'] == 'Sim']
    else:  # 'Sem Encaminhamento'
        df_filtrado = df_filtrado[df_filtrado['dado_algum_encaminhamento'] == 'N√£o']

# Mapeamento para nomes de colunas de institui√ß√µes
mapa_instituicoes = {
    'Educa√ß√£o': 'instituicao_encaminhamento_educacao',
    'Sa√∫de': 'instituicao_encaminhamento_saude',
    'Assist√™ncia Social': 'instituicao_encaminhamento_assistencia_social',
    'Conselho Tutelar': 'instituicao_encaminhamento_conselho_tutelar',
    'Esta√ß√£o Conhecimento': 'instituicao_encaminhamento_estacao_conhecimento',
    'Sociedade Civil': 'instituicao_encaminhamento_sociedade_civil',
    'Outro Equipamento': 'instituicao_encaminhamento_outro_equipamento'
}

if instituicao_selecionada != 'Todos':
    coluna_instituicao = mapa_instituicoes[instituicao_selecionada]
    df_filtrado = df_filtrado[df_filtrado[coluna_instituicao].notna()]

# TAB 1: VIS√ÉO GERAL
if tab_selecionada == "Vis√£o Geral":
    # Estat√≠sticas gerais
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total de Registros", len(df_filtrado))
    with col2:
        st.metric("Pontua√ß√£o M√©dia", f"{df_filtrado['pontuacao'].mean():.2f}/10")
    with col3:
        st.metric("Registros com Problemas", 
                 f"{df_filtrado['num_problemas'].gt(0).sum()} ({df_filtrado['num_problemas'].gt(0).mean()*100:.1f}%)")
    
    # Gr√°fico de distribui√ß√£o da qualidade - Vers√£o melhorada com Plotly
    st.subheader("Distribui√ß√£o da Classifica√ß√£o de Qualidade")
    
    # Preparando dados para o gr√°fico
    qualidade_counts = df_filtrado['classificacao_qualidade'].value_counts().reset_index()
    qualidade_counts.columns = ['classificacao', 'quantidade']
    
    # Definir ordem personalizada para categorias de qualidade
    ordem_qualidade = ["Excelente", "Bom", "Regular", "Ruim", "Cr√≠tico"]
    
    # Reordenar o DataFrame
    qualidade_counts['ordem'] = qualidade_counts['classificacao'].map({cat: i for i, cat in enumerate(ordem_qualidade)})
    qualidade_counts = qualidade_counts.sort_values('ordem').drop('ordem', axis=1)
    
    # Criar mapa de cores personalizado
    color_map = {
        'Excelente': '#1E88E5',
        'Bom': '#43A047',
        'Regular': '#FFB300',
        'Ruim': '#E53935',
        'Cr√≠tico': '#8E24AA'
    }
    
    # Criar gr√°fico de barras interativo melhorado
    fig = px.bar(
        qualidade_counts, 
        x='classificacao', 
        y='quantidade',
        color='classificacao',
        color_discrete_map=color_map,
        text='quantidade',
        labels={'classificacao': 'Classifica√ß√£o', 'quantidade': 'Quantidade de Registros'},
        height=500
    )
    
    # Personalizar layout
    fig.update_layout(
        title={
            'text': 'Distribui√ß√£o por Classifica√ß√£o de Qualidade',
            'y':0.95,
            'x':0.5,
            'xanchor': 'center',
            'yanchor': 'top',
            'font': {'size': 20}
        },
        xaxis_title="Classifica√ß√£o",
        yaxis_title="Quantidade de Registros",
        legend_title="Qualidade",
        hovermode="x unified",
        hoverlabel=dict(
            bgcolor="#333333", 
            bordercolor="#333333",
            font_size=14,
            font_family="Arial",
            font_color="white"
        ),
        showlegend=True
    )
    
    # Melhorar a exibi√ß√£o do texto nas barras e corrigir cores no hover
    fig.update_traces(
        textposition='outside',
        textfont=dict(size=14),
        hovertemplate='<b>%{x}</b><br>Registros: %{y}<extra></extra>'
    )
    
    # Definir cores personalizadas para cada barra individualmente
    for i, qualidade in enumerate(qualidade_counts['classificacao']):
        if qualidade in color_map:
            fig.data[0].marker.color = [color_map[q] for q in qualidade_counts['classificacao']]
            
    # Exibir o gr√°fico
    st.plotly_chart(fig, use_container_width=True)
    
    # Gr√°fico de distribui√ß√£o de comprimento
    st.subheader("Distribui√ß√£o do Comprimento das Descri√ß√µes")
    fig = px.histogram(df_filtrado, x='comprimento_descricao', nbins=50)
    fig.update_layout(
        xaxis_title="Comprimento (caracteres)", 
        yaxis_title="Quantidade de Registros",
        hoverlabel=dict(
            bgcolor="#333333", 
            bordercolor="#333333",
            font_size=14,
            font_family="Arial",
            font_color="white"
        )
    )
    st.plotly_chart(fig, use_container_width=True)
    
    # Tipos de problemas detectados
    st.subheader("Tipos de Problemas Detectados")
    
    # Desempacotando a lista de problemas para contar
    problemas_lista = []
    for problemas in df_filtrado['problemas_detectados']:
        problemas_lista.extend(problemas)
    
    problemas_contagem = pd.Series(problemas_lista).value_counts()
    
    if not problemas_contagem.empty:
        fig = px.bar(x=problemas_contagem.index, y=problemas_contagem.values,
                   labels={'x': 'Tipo de Problema', 'y': 'Quantidade de Ocorr√™ncias'})
        fig.update_layout(
            xaxis_title="Tipo de Problema",
            yaxis_title="Quantidade de Ocorr√™ncias", 
            hoverlabel=dict(
                bgcolor="#333333", 
                bordercolor="#333333",
                font_size=14,
                font_family="Arial",
                font_color="white"
            )
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Nenhum problema detectado nos registros filtrados.")

# TAB 2: AN√ÅLISE TEMPORAL
elif tab_selecionada == "An√°lise Temporal":
    st.subheader("An√°lise Temporal dos Registros")
    
    # Op√ß√µes de granularidade temporal
    opcao_tempo = st.radio(
        "Selecione a granularidade temporal:",
        ("Di√°rio", "Semanal", "Mensal"),
        horizontal=True
    )
    
    # Definir coluna de data baseada na op√ß√£o
    if opcao_tempo == "Di√°rio":
        coluna_tempo = 'acompanhamento_data'
        formato = '%d/%m/%Y'
    elif opcao_tempo == "Semanal":
        coluna_tempo = 'semana'
        formato = 'Semana %U, %Y'
    else:  # Mensal
        coluna_tempo = 'mes_ano'
        formato = '%b %Y'
    
    # An√°lise de volume ao longo do tempo
    st.markdown("### Volume de Registros ao Longo do Tempo")
    
    # Contar registros por per√≠odo
    df_tempo = df_filtrado.groupby(coluna_tempo).size().reset_index(name='quantidade')
    
    # Ordenar por data
    df_tempo = df_tempo.sort_values(coluna_tempo)
    
    # Gr√°fico de linha para volume
    fig_volume = px.line(
        df_tempo, 
        x=coluna_tempo, 
        y='quantidade',
        labels={'quantidade': 'Quantidade de Registros', coluna_tempo: 'Per√≠odo'},
        markers=True
    )
    
    # Personalizar layout
    fig_volume.update_layout(
        xaxis_title="Per√≠odo",
        yaxis_title="Quantidade de Registros",
        height=500
    )
    
    # Se estiver usando mes_ano, substituir os valores do eixo x para exibi√ß√£o
    if coluna_tempo == 'mes_ano':
        # Criar um dicion√°rio de mapeamento de mes_ano para mes_ano_display
        mapeamento_mes = dict(zip(df_filtrado['mes_ano'], df_filtrado['mes_ano_display']))
        # Aplicar o mapeamento aos tickets do eixo x
        fig_volume.update_xaxes(
            tickvals=list(mapeamento_mes.keys()),
            ticktext=list(mapeamento_mes.values())
        )
    
    st.plotly_chart(fig_volume, use_container_width=True)
    
    # NOVA SE√á√ÉO: Volume de acompanhamentos por articulador ao longo do tempo
    st.markdown("### Volume por Articulador ao Longo do Tempo")
    
    # Op√ß√µes de visualiza√ß√£o
    tipo_viz = st.radio(
        "Tipo de visualiza√ß√£o:",
        ("Gr√°fico de Linhas", "Gr√°fico de Barras Empilhadas", "Mapa de Calor"),
        horizontal=True
    )
    
    # Agrupar dados por per√≠odo e articulador
    df_volume_articulador = df_filtrado.groupby([coluna_tempo, 'acompanhamento_articulador']).size().reset_index(name='quantidade')
    
    # Ordenar por data
    df_volume_articulador = df_volume_articulador.sort_values(coluna_tempo)
    
    if tipo_viz == "Gr√°fico de Linhas":
        # Gr√°fico de linha para cada articulador
        fig_articulador = px.line(
            df_volume_articulador,
            x=coluna_tempo,
            y='quantidade',
            color='acompanhamento_articulador',
            labels={
                'quantidade': 'Quantidade de Registros', 
                coluna_tempo: 'Per√≠odo',
                'acompanhamento_articulador': 'Articulador'
            },
            markers=True
        )
        
        # Personalizar layout
        fig_articulador.update_layout(
            xaxis_title="Per√≠odo",
            yaxis_title="Quantidade de Registros",
            height=600,
            legend_title="Articulador"
        )
        
        # Se estiver usando mes_ano, substituir os valores do eixo x para exibi√ß√£o
        if coluna_tempo == 'mes_ano':
            # Criar um dicion√°rio de mapeamento de mes_ano para mes_ano_display
            mapeamento_mes = dict(zip(df_filtrado['mes_ano'], df_filtrado['mes_ano_display']))
            # Aplicar o mapeamento aos tickets do eixo x
            fig_articulador.update_xaxes(
                tickvals=list(mapeamento_mes.keys()),
                ticktext=list(mapeamento_mes.values())
            )
    
    elif tipo_viz == "Gr√°fico de Barras Empilhadas":
        # Gr√°fico de barras empilhadas
        fig_articulador = px.bar(
            df_volume_articulador,
            x=coluna_tempo,
            y='quantidade',
            color='acompanhamento_articulador',
            labels={
                'quantidade': 'Quantidade de Registros', 
                coluna_tempo: 'Per√≠odo',
                'acompanhamento_articulador': 'Articulador'
            }
        )
        
        # Personalizar layout
        fig_articulador.update_layout(
            xaxis_title="Per√≠odo",
            yaxis_title="Quantidade de Registros",
            height=600,
            legend_title="Articulador"
        )
        
        # Se estiver usando mes_ano, substituir os valores do eixo x para exibi√ß√£o
        if coluna_tempo == 'mes_ano':
            # Criar um dicion√°rio de mapeamento de mes_ano para mes_ano_display
            mapeamento_mes = dict(zip(df_filtrado['mes_ano'], df_filtrado['mes_ano_display']))
            # Aplicar o mapeamento aos tickets do eixo x
            fig_articulador.update_xaxes(
                tickvals=list(mapeamento_mes.keys()),
                ticktext=list(mapeamento_mes.values())
            )
    
    else:  # Mapa de Calor
        # Ordenar os dados por per√≠odo antes de criar o pivot
        df_volume_articulador = df_volume_articulador.sort_values(coluna_tempo)
        
        # Se estiver usando mes_ano, criar um dicion√°rio de mapeamento para exibi√ß√£o
        label_mapping = {}
        if coluna_tempo == 'mes_ano':
            label_mapping = dict(zip(df_filtrado['mes_ano'], df_filtrado['mes_ano_display']))
        
        # Criar tabela pivot para o mapa de calor
        pivot_data = df_volume_articulador.pivot_table(
            index='acompanhamento_articulador', 
            columns=coluna_tempo, 
            values='quantidade',
            fill_value=0
        )
        
        # Ordenar colunas cronologicamente
        pivot_data = pivot_data.reindex(sorted(pivot_data.columns), axis=1)
        
        # Mapear nomes das colunas para exibi√ß√£o se necess√°rio
        column_labels = pivot_data.columns
        if coluna_tempo == 'mes_ano' and label_mapping:
            column_labels = [label_mapping.get(col, col) for col in pivot_data.columns]
        
        # Criar mapa de calor
        fig_articulador = px.imshow(
            pivot_data,
            labels=dict(x="Per√≠odo", y="Articulador", color="Quantidade"),
            x=column_labels,  # Usar as etiquetas mapeadas
            y=pivot_data.index,
            color_continuous_scale='viridis',
            text_auto=True
        )
        
        # Personalizar layout
        fig_articulador.update_layout(
            height=25 * (len(pivot_data) + 5),  # Altura adaptativa
            xaxis_title="Per√≠odo",
            yaxis_title="Articulador"
        )
    
    st.plotly_chart(fig_articulador, use_container_width=True)
    
    # An√°lise estat√≠stica b√°sica da distribui√ß√£o temporal
    with st.expander("Estat√≠sticas da Distribui√ß√£o Temporal"):
        # Identificar per√≠odos sem atividade
        st.markdown("**Distribui√ß√£o de registros por articulador:**")
        dist_articulador = df_filtrado.groupby('acompanhamento_articulador').size().reset_index(name='quantidade')
        dist_articulador = dist_articulador.sort_values('quantidade', ascending=False)
        
        # Gr√°fico de barras para distribui√ß√£o por articulador
        fig_dist = px.bar(
            dist_articulador,
            x='acompanhamento_articulador',
            y='quantidade',
            labels={
                'quantidade': 'Quantidade de Registros',
                'acompanhamento_articulador': 'Articulador'
            }
        )
        
        # Aplicar tema escuro aos tooltips
        fig_dist.update_layout(
            hoverlabel=dict(
                bgcolor="#333333", 
                bordercolor="#333333",
                font_size=14,
                font_family="Arial",
                font_color="white"
            )
        )
        
        st.plotly_chart(fig_dist, use_container_width=True)
        
        # Tabela de contagem por articulador
        st.markdown("**Contagem de registros por articulador:**")
        st.dataframe(
            dist_articulador.rename(columns={
                'acompanhamento_articulador': 'Articulador',
                'quantidade': 'Quantidade de Registros'
            }).set_index('Articulador'),
            use_container_width=True
        )
    
    # An√°lise da qualidade dos registros ao longo do tempo
    st.markdown("### Qualidade dos Registros ao Longo do Tempo")
    
    # Qualidade m√©dia por m√™s
    qualidade_por_mes = df_filtrado.groupby('mes_ano')['pontuacao'].mean().reset_index()
    qualidade_por_mes = qualidade_por_mes.sort_values('mes_ano')
    
    if not qualidade_por_mes.empty:
        st.subheader("Evolu√ß√£o da Qualidade dos Registros ao Longo do Tempo")
        fig = px.line(qualidade_por_mes, x='mes_ano', y='pontuacao',
                    labels={'mes_ano': 'M√™s/Ano', 'pontuacao': 'Pontua√ß√£o M√©dia'},
                    markers=True)
        fig.update_layout(xaxis_title="Per√≠odo", yaxis_title="Pontua√ß√£o M√©dia (0-10)")
        st.plotly_chart(fig, use_container_width=True)
    
    # Distribui√ß√£o de problemas por m√™s
    st.subheader("Distribui√ß√£o de Problemas por Per√≠odo")
    
    # Calcular percentual de registros problem√°ticos por m√™s
    problemas_por_mes = df_filtrado.groupby('mes_ano')['num_problemas'].apply(
        lambda x: (x > 0).mean() * 100).reset_index()
    problemas_por_mes.columns = ['mes_ano', 'pct_problematicos']
    problemas_por_mes = problemas_por_mes.sort_values('mes_ano')
    
    if not problemas_por_mes.empty:
        fig = px.bar(problemas_por_mes, x='mes_ano', y='pct_problematicos',
                   labels={'mes_ano': 'M√™s/Ano', 'pct_problematicos': '% de Registros com Problemas'},
                   color='pct_problematicos', color_continuous_scale='RdYlGn_r')
        fig.update_layout(xaxis_title="Per√≠odo", yaxis_title="% de Registros com Problemas")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("N√£o h√° dados suficientes para a an√°lise de problemas por per√≠odo.")
    
    # Dia da semana com mais registros
    if len(df_filtrado) > 0:
        st.subheader("Distribui√ß√£o por Dia da Semana")
        df_filtrado['dia_semana'] = df_filtrado['acompanhamento_data'].dt.day_name()
        dias_ordem = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        dias_portugues = ['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo']
        
        # Criar um mapeamento para dias da semana em portugu√™s
        mapa_dias = dict(zip(dias_ordem, dias_portugues))
        df_filtrado['dia_semana_pt'] = df_filtrado['dia_semana'].map(mapa_dias)
        
        registros_por_dia = df_filtrado.groupby('dia_semana_pt').size().reset_index(name='contagem')
        # Ordenar os dias da semana corretamente
        registros_por_dia['ordem'] = registros_por_dia['dia_semana_pt'].map(
            dict(zip(dias_portugues, range(7))))
        registros_por_dia = registros_por_dia.sort_values('ordem')
        
        fig = px.bar(registros_por_dia, x='dia_semana_pt', y='contagem',
                   labels={'dia_semana_pt': 'Dia da Semana', 'contagem': 'Quantidade de Registros'},
                   color='contagem')
        fig.update_layout(xaxis_title="Dia da Semana", yaxis_title="Quantidade de Registros")
        st.plotly_chart(fig, use_container_width=True)

# TAB 3: AN√ÅLISE POR ARTICULADOR
elif tab_selecionada == "An√°lise por Articulador":
    # Agrupando dados por articulador
    grupo_articulador = df_filtrado.groupby('acompanhamento_articulador').agg({
        'acompanhamento_descricao': 'count',
        'pontuacao': 'mean',
        'num_problemas': 'mean',
        'comprimento_descricao': 'mean'
    }).reset_index()
    
    grupo_articulador = grupo_articulador.rename(columns={
        'acompanhamento_articulador': 'articulador',
        'acompanhamento_descricao': 'total_registros',
        'pontuacao': 'pontuacao_media',
        'num_problemas': 'media_problemas',
        'comprimento_descricao': 'comprimento_medio'
    })
    
    # Calcular percentual de registros problem√°ticos por articulador
    problemas_por_articulador = df_filtrado.groupby('acompanhamento_articulador')['num_problemas'].apply(
        lambda x: (x > 0).mean() * 100).reset_index()
    problemas_por_articulador.columns = ['articulador', 'pct_registros_problematicos']
    
    # Juntar os dados
    grupo_articulador = pd.merge(grupo_articulador, problemas_por_articulador, on='articulador')
    
    # Ordenar por pontua√ß√£o m√©dia (decrescente)
    grupo_articulador = grupo_articulador.sort_values('pontuacao_media', ascending=False)
    
    # Gr√°fico de pontua√ß√£o m√©dia por articulador
    st.subheader("Pontua√ß√£o M√©dia por Articulador")
    if not grupo_articulador.empty:
        fig = px.bar(grupo_articulador, x='articulador', y='pontuacao_media',
                labels={'articulador': 'Articulador', 'pontuacao_media': 'Pontua√ß√£o M√©dia'},
                color='pontuacao_media', color_continuous_scale='RdYlGn')
        fig.update_layout(xaxis_title="Articulador", yaxis_title="Pontua√ß√£o M√©dia (0-10)")
        st.plotly_chart(fig, use_container_width=True)
        
        # Gr√°fico de percentual de registros problem√°ticos por articulador
        st.subheader("% de Registros com Problemas por Articulador")
        fig = px.bar(grupo_articulador.sort_values('pct_registros_problematicos'), 
                x='articulador', y='pct_registros_problematicos',
                labels={'articulador': 'Articulador', 
                        'pct_registros_problematicos': '% de Registros com Problemas'},
                color='pct_registros_problematicos', color_continuous_scale='RdYlGn_r')
        fig.update_layout(xaxis_title="Articulador", yaxis_title="% de Registros com Problemas")
        st.plotly_chart(fig, use_container_width=True)
        
        # Gr√°fico de volume de registros por articulador
        st.subheader("Volume de Registros por Articulador")
        fig = px.bar(grupo_articulador.sort_values('total_registros', ascending=False), 
                x='articulador', y='total_registros',
                labels={'articulador': 'Articulador', 
                        'total_registros': 'Total de Registros'},
                color='total_registros')
        fig.update_layout(xaxis_title="Articulador", yaxis_title="Total de Registros")
        st.plotly_chart(fig, use_container_width=True)
        
        # Tabela com dados por articulador
        st.subheader("Dados Detalhados por Articulador")
        st.dataframe(grupo_articulador)
    else:
        st.info("N√£o h√° dados suficientes para an√°lise por articulador com os filtros atuais.")

# TAB 4: EXEMPLOS DE PROBLEMAS
elif tab_selecionada == "Exemplos de Problemas":
    # Definir os tipos de problemas para mostrar exemplos
    tipos_problemas_exemplos = ['muito_curto', 'vago', 'sem_detalhes', 'generico']
    problema_para_exemplos = st.selectbox(
        'Selecione um tipo de problema para ver exemplos:',
        tipos_problemas_exemplos
    )
    
    # Filtrar registros com o problema selecionado
    exemplos_df = df_filtrado[df_filtrado['problemas_detectados'].apply(lambda x: problema_para_exemplos in x)]
    
    if len(exemplos_df) > 0:
        st.subheader(f"Exemplos de Registros com o Problema: '{problema_para_exemplos}'")
        
        # Ordenar por comprimento da descri√ß√£o (crescente)
        exemplos_df = exemplos_df.sort_values('comprimento_descricao')
        
        # Mostrar exemplos
        for i, (idx, row) in enumerate(exemplos_df.head(5).iterrows()):
            with st.expander(f"Exemplo {i+1}: {row['acompanhamento_articulador']} - {row['acompanhamento_data'].strftime('%d/%m/%Y')}"):
                st.markdown(f"**ID do Cadastro:** {row['id']}")
                st.markdown(f"**C√≥digo do Acompanhamento:** {row['acompanhamento_cod']}")
                st.markdown(f"**Articulador:** {row['acompanhamento_articulador']}")
                st.markdown(f"**Data:** {row['acompanhamento_data'].strftime('%d/%m/%Y')}")
                st.markdown(f"**Descri√ß√£o:**")
                st.text_area("Descri√ß√£o", row['acompanhamento_descricao'], height=100, key=f"descricao_problema_{problema_para_exemplos}_{i}", disabled=True, label_visibility="collapsed")
                
                # Mostrar outros problemas detectados
                outros_problemas = [p for p in row['problemas_detectados'] if p != problema_para_exemplos]
                if outros_problemas:
                    st.markdown(f"**Outros problemas detectados:** {', '.join(outros_problemas)}")
                
                # Mostrar qualidade calculada
                st.markdown(f"**Qualidade calculada:** {row['classificacao_qualidade']} (Pontua√ß√£o: {row['pontuacao']}/10)")
    else:
        st.info(f"Nenhum registro encontrado com o problema '{problema_para_exemplos}'.")

# TAB 5: AN√ÅLISE DE TEXTO
elif tab_selecionada == "An√°lise de Texto":
    # An√°lise de texto
    st.subheader("An√°lise das Palavras mais Frequentes")
    
    # Amostra para an√°lise de texto (para evitar sobrecarga)
    amostra = df_filtrado.head(1000) if len(df_filtrado) > 1000 else df_filtrado
    
    # Juntar todos os textos pr√©-processados
    textos_combinados = ' '.join(amostra['texto_preprocessado'].dropna())
    
    if textos_combinados.strip():
        # Criar nuvem de palavras
        wordcloud = WordCloud(
            width=800, 
            height=400, 
            background_color='white',
            colormap='viridis',
            max_words=100,
            contour_width=1,
            contour_color='steelblue',
            collocations=False
        ).generate(textos_combinados)
        
        # Exibir a nuvem de palavras
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        st.pyplot(fig)
        
        # An√°lise de frequ√™ncia de palavras
        st.subheader("Palavras mais Frequentes")
        
        # Extrair palavras para an√°lise de frequ√™ncia
        palavras = textos_combinados.split()
        freq_palavras = pd.Series(palavras).value_counts().head(20)
        
        # Plotar gr√°fico de barras com as palavras mais frequentes
        fig = px.bar(x=freq_palavras.index, y=freq_palavras.values,
                   labels={'x': 'Palavra', 'y': 'Frequ√™ncia'})
        fig.update_layout(
            xaxis_title="Palavra", 
            yaxis_title="Frequ√™ncia",
            hoverlabel=dict(
                bgcolor="#333333", 
                bordercolor="#333333",
                font_size=14,
                font_family="Arial",
                font_color="white"
            )
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("N√£o h√° dados suficientes para an√°lise de texto com os filtros atuais.")

# TAB 6: CONSIST√äNCIA DE SUCESSO
elif tab_selecionada == "Consist√™ncia de Sucesso":
    st.subheader("An√°lise de Consist√™ncia entre Sucesso de Contato e Descri√ß√£o")
    
    with st.expander("üìã Entenda a An√°lise de Consist√™ncia", expanded=True):
        st.markdown("""
        ### O que √© a an√°lise de consist√™ncia?
        
        Esta an√°lise verifica se h√° coer√™ncia entre:
        
        1. **Sucesso de Contato**: Refere-se ao fato de ter conseguido entrar em contato com a pessoa ou institui√ß√£o. Exemplos de sucesso: "falei com a m√£e", "em contato com a diretora", "de acordo com a secretaria".
        
        2. **Conte√∫do da Descri√ß√£o**: O texto que explica o que ocorreu durante o acompanhamento.
        
        A inconsist√™ncia ocorre quando:
        - A coluna de sucesso indica "Sim", mas a descri√ß√£o sugere que n√£o houve sucesso (ex: "n√£o atendeu", "n√∫mero inexistente")
        - A coluna de sucesso indica "N√£o", mas a descri√ß√£o sugere que houve contato (ex: "m√£e informou que...", "conversando com respons√°vel")
        
        Uma alta taxa de inconsist√™ncia pode indicar problemas no entendimento dos campos ou no processo de registro.
        """)
    
    # Criar dataframe com poss√≠veis inconsist√™ncias
    df_inconsistente = df_filtrado[df_filtrado['consistencia_sucesso'].isin(['Poss√≠vel Inconsist√™ncia (Sim/Insucesso)', 'Poss√≠vel Inconsist√™ncia (N√£o/Sucesso)'])].copy()
    
    # Adicionar coluna com tipo de inconsist√™ncia
    df_inconsistente['tipo_inconsistencia'] = df_inconsistente.apply(
        lambda row: "Marcado como 'Sim' mas descri√ß√£o indica insucesso" if row['acompanhamento_sucesso_contato'] == 'Sim'
        else "Marcado como 'N√£o' mas descri√ß√£o indica sucesso", axis=1
    )
    
    # An√°lise geral
    col1, col2, col3 = st.columns(3)
    
    with col1:
        total_registros = len(df_filtrado)
        total_inconsistencias = len(df_inconsistente)
        percentual = (total_inconsistencias / total_registros * 100) if total_registros > 0 else 0
        st.metric("Registros Inconsistentes", f"{total_inconsistencias} ({percentual:.1f}%)")
        
    with col2:
        # Tipo 1: Marcado como "Sim" mas descri√ß√£o indica insucesso
        tipo1 = len(df_inconsistente[df_inconsistente['tipo_inconsistencia'] == "Marcado como 'Sim' mas descri√ß√£o indica insucesso"])
        percentual_tipo1 = (tipo1 / total_inconsistencias * 100) if total_inconsistencias > 0 else 0
        st.metric("Falsos Sucessos", f"{tipo1} ({percentual_tipo1:.1f}%)")
        
    with col3:
        # Tipo 2: Marcado como "N√£o" mas descri√ß√£o indica sucesso
        tipo2 = len(df_inconsistente[df_inconsistente['tipo_inconsistencia'] == "Marcado como 'N√£o' mas descri√ß√£o indica sucesso"])
        percentual_tipo2 = (tipo2 / total_inconsistencias * 100) if total_inconsistencias > 0 else 0
        st.metric("Falsos Insucessos", f"{tipo2} ({percentual_tipo2:.1f}%)")
    
    # An√°lise por articulador
    st.subheader("Consist√™ncia por Articulador")
    
    # Criar dataframe com contagem de inconsist√™ncias por articulador
    df_consistencia_articulador = pd.DataFrame()
    
    if not df_filtrado.empty:
        # Agrupar e contar por articulador
        total_por_articulador = df_filtrado.groupby('acompanhamento_articulador').size().reset_index(name='total_registros')
        inconsistentes_por_articulador = df_inconsistente.groupby('acompanhamento_articulador').size().reset_index(name='registros_inconsistentes')
        
        # Mesclar os dataframes
        df_consistencia_articulador = total_por_articulador.merge(
            inconsistentes_por_articulador, 
            on='acompanhamento_articulador', 
            how='left'
        ).fillna(0)
        
        # Calcular percentual
        df_consistencia_articulador['registros_inconsistentes'] = df_consistencia_articulador['registros_inconsistentes'].astype(int)
        df_consistencia_articulador['percentual_inconsistencia'] = (df_consistencia_articulador['registros_inconsistentes'] / 
                                                                df_consistencia_articulador['total_registros'] * 100)
        
        # Ordenar por percentual de inconsist√™ncia (decrescente)
        df_consistencia_articulador = df_consistencia_articulador.sort_values('percentual_inconsistencia', ascending=False)
        
        # Visualiza√ß√£o em gr√°fico
        col1, col2 = st.columns(2)
        
        with col1:
            # Gr√°fico de barras para percentual de inconsist√™ncia
            fig_percentual = px.bar(
                df_consistencia_articulador,
                x='acompanhamento_articulador',
                y='percentual_inconsistencia',
                labels={
                    'acompanhamento_articulador': 'Articulador',
                    'percentual_inconsistencia': '% de Inconsist√™ncias'
                },
                text_auto='.1f',
                title='Percentual de Inconsist√™ncias por Articulador'
            )
            
            fig_percentual.update_layout(xaxis_tickangle=-45, height=500)
            st.plotly_chart(fig_percentual, use_container_width=True)
        
        with col2:
            # Tabela com dados de inconsist√™ncia por articulador
            st.dataframe(
                df_consistencia_articulador.rename(columns={
                    'acompanhamento_articulador': 'Articulador',
                    'total_registros': 'Total de Registros',
                    'registros_inconsistentes': 'Registros Inconsistentes',
                    'percentual_inconsistencia': '% de Inconsist√™ncia'
                }).set_index('Articulador'),
                use_container_width=True
            )
        
        # An√°lise de tipos de inconsist√™ncia por articulador
        st.subheader("Tipos de Inconsist√™ncia por Articulador")
        
        # Criar pivot table com tipos de inconsist√™ncia por articulador
        pivot_inconsistencia = pd.crosstab(
            df_inconsistente['acompanhamento_articulador'],
            df_inconsistente['tipo_inconsistencia'],
            normalize='index'
        ) * 100
        
        # Visualizar como heatmap
        fig_heatmap = px.imshow(
            pivot_inconsistencia,
            text_auto='.1f',
            labels=dict(x="Tipo de Inconsist√™ncia", y="Articulador", color="Percentual (%)"),
            x=pivot_inconsistencia.columns,
            y=pivot_inconsistencia.index,
            color_continuous_scale='RdYlGn_r',
            aspect="auto"
        )
        
        fig_heatmap.update_layout(height=25 * (len(pivot_inconsistencia) + 5))
        st.plotly_chart(fig_heatmap, use_container_width=True)
    
    else:
        st.info("N√£o h√° dados suficientes para an√°lise com os filtros atuais.")
    
    # Exibir exemplos de poss√≠veis inconsist√™ncias
    if not df_inconsistente.empty:
        st.subheader(f"Registros com Poss√≠veis Inconsist√™ncias ({len(df_inconsistente)})")
        
        # Agrupar por tipo de inconsist√™ncia
        for tipo_inconsistencia in df_inconsistente['tipo_inconsistencia'].unique():
            df_tipo = df_inconsistente[df_inconsistente['tipo_inconsistencia'] == tipo_inconsistencia]
            
            st.markdown(f"### {tipo_inconsistencia} ({len(df_tipo)} registros)")
            
            # Mostrar alguns exemplos
            for i, (idx, row) in enumerate(df_tipo.head(3).iterrows()):
                with st.expander(f"Exemplo {i+1}: {row['acompanhamento_articulador']} - {row['acompanhamento_data'].strftime('%d/%m/%Y')}"):
                    st.markdown(f"**ID do Cadastro:** {row['id']}")
                    st.markdown(f"**C√≥digo do Acompanhamento:** {row['acompanhamento_cod']}")
                    st.markdown(f"**Articulador:** {row['acompanhamento_articulador']}")
                    st.markdown(f"**Data:** {row['acompanhamento_data'].strftime('%d/%m/%Y')}")
                    st.markdown(f"**Status Reportado:** {row['acompanhamento_sucesso_contato']}")
                    st.markdown(f"**Descri√ß√£o:**")
                    st.text_area(f"Descri√ß√£o {tipo_inconsistencia} {i}", row['acompanhamento_descricao'], height=100, disabled=True, label_visibility="collapsed")
                    
                    # Mostrar palavras detectadas que causaram a inconsist√™ncia
                    if 'palavras_detectadas' in row and row['palavras_detectadas']:
                        st.markdown(f"**Termos detectados:** {', '.join(row['palavras_detectadas'])}")

# TAB 7: AN√ÅLISE DE ENCAMINHAMENTOS
elif tab_selecionada == "An√°lise de Encaminhamentos":
    st.subheader("An√°lise de Encaminhamentos")
    
    # Definir colunas de institui√ß√µes para uso nesta aba
    colunas_instituicoes = [
        'instituicao_encaminhamento_educacao',
        'instituicao_encaminhamento_saude',
        'instituicao_encaminhamento_assistencia_social',
        'instituicao_encaminhamento_conselho_tutelar',
        'instituicao_encaminhamento_estacao_conhecimento',
        'instituicao_encaminhamento_sociedade_civil',
        'instituicao_encaminhamento_outro_equipamento'
    ]
    
    # Adicionar explica√ß√£o sobre a an√°lise de encaminhamentos
    with st.expander("üìã Entenda a An√°lise de Encaminhamentos", expanded=True):
        st.markdown("""
        ### O que s√£o encaminhamentos?
        
        Encaminhamento √© a orienta√ß√£o formal do projeto TR para que a fam√≠lia procure ou compare√ßa em alguma institui√ß√£o da rede local parceira para atendimento. S√£o os encaminhamentos dados para as unidades escolares, equipamentos p√∫blicos de sa√∫de e de assist√™ncia social, organiza√ß√µes locais e outros.
        
        S√£o classificados de acordo com o tipo de institui√ß√£o para qual o encaminhamento foi realizado:
        
        - **Educa√ß√£o**: escolas, secretarias de educa√ß√£o, etc.
        - **Sa√∫de**: unidades de sa√∫de, hospitais, etc.
        - **Assist√™ncia Social**: CRAS, CREAS, etc.
        - **Conselho Tutelar**: √≥rg√£os de prote√ß√£o √† crian√ßa e adolescente.
        - **Esta√ß√£o Conhecimento**: programas espec√≠ficos de forma√ß√£o.
        - **Sociedade Civil**: ONGs, associa√ß√µes comunit√°rias, etc.
        - **Outros Equipamentos**: demais servi√ßos p√∫blicos ou privados.
        
        A an√°lise de encaminhamentos permite compreender o fluxo de atendimento e as redes de apoio mais acionadas pelos articuladores.
        """)
    
    # M√©tricas gerais de encaminhamentos
    st.markdown("### M√©tricas Gerais de Encaminhamentos")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        total_registros = len(df_filtrado)
        total_com_encaminhamento = df_filtrado[df_filtrado['dado_algum_encaminhamento'] == 'Sim'].shape[0]
        pct_com_encaminhamento = (total_com_encaminhamento / total_registros) * 100 if total_registros > 0 else 0
        st.metric("Registros com Encaminhamento", f"{total_com_encaminhamento} ({pct_com_encaminhamento:.1f}%)")
    
    with col2:
        # Calcular m√©dia de institui√ß√µes encaminhadas por registro
        if total_com_encaminhamento > 0:
            media_instituicoes = df_filtrado[df_filtrado['dado_algum_encaminhamento'] == 'Sim']['num_instituicoes_encaminhadas'].mean()
            st.metric("M√©dia de Institui√ß√µes por Encaminhamento", f"{media_instituicoes:.1f}")
        else:
            st.metric("M√©dia de Institui√ß√µes por Encaminhamento", "0")
    
    with col3:
        # Identificar institui√ß√£o mais encaminhada
        if total_com_encaminhamento > 0:
            # Contar encaminhamentos por tipo de institui√ß√£o
            contagem_instituicoes = {}
            total_encaminhamentos = 0
            
            for coluna in colunas_instituicoes:
                tipo = coluna.replace('instituicao_encaminhamento_', '')
                contagem = df_filtrado[df_filtrado[coluna].notna()].shape[0]
                contagem_instituicoes[tipo] = contagem
                total_encaminhamentos += contagem
            
            # Encontrar a mais comum
            if contagem_instituicoes:
                instituicao_mais_comum = max(contagem_instituicoes.items(), key=lambda x: x[1])
                pct_mais_comum = (instituicao_mais_comum[1] / total_encaminhamentos) * 100 if total_encaminhamentos > 0 else 0
                st.metric("Institui√ß√£o Mais Encaminhada", f"{instituicao_mais_comum[0]} ({pct_mais_comum:.1f}%)")
            else:
                st.metric("Institui√ß√£o Mais Encaminhada", "Nenhuma")
        else:
            st.metric("Institui√ß√£o Mais Encaminhada", "Nenhuma")
    
    # Visualiza√ß√£o da distribui√ß√£o de encaminhamentos
    st.markdown("### Distribui√ß√£o de Encaminhamentos por Tipo de Institui√ß√£o")
    
    # Contar cada tipo de institui√ß√£o
    if total_com_encaminhamento > 0:
        # Criar um DataFrame com a contagem de cada tipo de institui√ß√£o
        dados_instituicoes = []
        for coluna in colunas_instituicoes:
            tipo = coluna.replace('instituicao_encaminhamento_', '')
            contagem = df_filtrado[df_filtrado[coluna].notna()].shape[0]
            if contagem > 0:
                dados_instituicoes.append({'tipo': tipo, 'contagem': contagem})
        
        df_instituicoes = pd.DataFrame(dados_instituicoes)
        
        if not df_instituicoes.empty:
            # Ordenar por contagem decrescente
            df_instituicoes = df_instituicoes.sort_values('contagem', ascending=False)
            
            # Criar gr√°fico de barras
            fig = px.bar(
                df_instituicoes, 
                x='tipo', 
                y='contagem',
                labels={'tipo': 'Tipo de Institui√ß√£o', 'contagem': 'Quantidade de Encaminhamentos'},
                color='tipo',
                text='contagem'
            )
            
            fig.update_layout(xaxis_title="Tipo de Institui√ß√£o", yaxis_title="Quantidade")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("N√£o h√° dados suficientes para an√°lise de distribui√ß√£o de encaminhamentos.")
    else:
        st.info("N√£o h√° encaminhamentos nos dados filtrados.")
    
    # An√°lise por Articulador
    st.markdown("### Encaminhamentos por Articulador")
    
    # Calcular percentual de registros com encaminhamento por articulador
    encaminhamentos_por_articulador = df_filtrado.groupby('acompanhamento_articulador')['dado_algum_encaminhamento'].value_counts().unstack().fillna(0)
    
    if not encaminhamentos_por_articulador.empty and 'Sim' in encaminhamentos_por_articulador.columns:
        # Calcular total de registros por articulador
        encaminhamentos_por_articulador['total'] = encaminhamentos_por_articulador.sum(axis=1)
        
        # Calcular percentual
        encaminhamentos_por_articulador['percentual_sim'] = (encaminhamentos_por_articulador['Sim'] / encaminhamentos_por_articulador['total']) * 100
        
        # Criar DataFrame para visualiza√ß√£o
        df_viz = encaminhamentos_por_articulador.reset_index()
        df_viz = df_viz.sort_values('percentual_sim', ascending=False)
        
        # Criar gr√°fico de barras
        fig = px.bar(
            df_viz,
            x='acompanhamento_articulador',
            y='percentual_sim',
            title="Percentual de Registros com Encaminhamento por Articulador",
            labels={'acompanhamento_articulador': 'Articulador', 'percentual_sim': '% de Registros com Encaminhamento'},
            text=df_viz['percentual_sim'].round(1).astype(str) + '%',
            color='percentual_sim'
        )
        
        fig.update_layout(xaxis_title="Articulador", yaxis_title="% de Registros com Encaminhamento")
        st.plotly_chart(fig, use_container_width=True)
        
        # Verificar distribui√ß√£o de tipos de institui√ß√£o por articulador
        st.markdown("### Matriz de Encaminhamentos por Articulador e Tipo de Institui√ß√£o")
        
        # Criar matriz de articuladores x tipos de institui√ß√£o
        dados_matriz = []
        articuladores = df_filtrado['acompanhamento_articulador'].unique()
        
        for articulador in articuladores:
            df_art = df_filtrado[df_filtrado['acompanhamento_articulador'] == articulador]
            
            for coluna in colunas_instituicoes:
                tipo = coluna.replace('instituicao_encaminhamento_', '')
                contagem = df_art[df_art[coluna].notna()].shape[0]
                total_reg_articulador = len(df_art)
                percentual = (contagem / total_reg_articulador) * 100 if total_reg_articulador > 0 else 0
                
                dados_matriz.append({
                    'articulador': articulador,
                    'tipo_instituicao': tipo,
                    'contagem': contagem,
                    'percentual': percentual
                })
        
        df_matriz = pd.DataFrame(dados_matriz)
        
        if not df_matriz.empty:
            # Criar heatmap
            pivot_table = df_matriz.pivot_table(
                values='contagem',
                index='articulador',
                columns='tipo_instituicao',
                fill_value=0
            )
            
            # Normalizar por linha (por articulador)
            pivot_norm = pivot_table.div(pivot_table.sum(axis=1), axis=0) * 100
            
            # Criar heatmap com plotly
            fig = px.imshow(
                pivot_norm,
                labels=dict(x="Tipo de Institui√ß√£o", y="Articulador", color="% do Total"),
                x=pivot_norm.columns,
                y=pivot_norm.index,
                aspect="auto",
                color_continuous_scale="YlGnBu"
            )
            
            fig.update_layout(
                xaxis_title="Tipo de Institui√ß√£o",
                yaxis_title="Articulador",
                coloraxis_colorbar=dict(
                    title="% do Total<br>de Encaminhamentos",
                    ticksuffix="%"
                )
            )
            
            st.plotly_chart(fig, use_container_width=True)
            
            # An√°lise de institui√ß√µes espec√≠ficas
            st.markdown("### An√°lise de Institui√ß√µes Espec√≠ficas")
            
            # Permitir ao usu√°rio selecionar um tipo de institui√ß√£o
            tipo_inst_selecionado = st.selectbox(
                "Selecione um tipo de institui√ß√£o para an√°lise detalhada:",
                options=[c.replace('instituicao_encaminhamento_', '') for c in colunas_instituicoes],
                key="select_tipo_instituicao_2"
            )
            
            coluna_selecionada = f"instituicao_encaminhamento_{tipo_inst_selecionado}"
            
            # Verificar se existe a coluna
            if coluna_selecionada in df_filtrado.columns:
                # Contar as institui√ß√µes mais comuns deste tipo
                contagem_inst = df_filtrado[coluna_selecionada].value_counts().reset_index()
                contagem_inst.columns = ['instituicao', 'contagem']
                
                # Remover valores nulos
                contagem_inst = contagem_inst[contagem_inst['instituicao'].notna()]
                
                if not contagem_inst.empty:
                    # Ordenar por contagem
                    contagem_inst = contagem_inst.sort_values('contagem', ascending=False)
                    
                    # Limitar a 15 para visualiza√ß√£o
                    contagem_inst_viz = contagem_inst.head(15)
                    
                    # Criar gr√°fico de barras
                    fig = px.bar(
                        contagem_inst_viz,
                        x='instituicao',
                        y='contagem',
                        title=f"Institui√ß√µes Mais Comuns do Tipo: {tipo_inst_selecionado}",
                        labels={'instituicao': 'Nome da Institui√ß√£o', 'contagem': 'Quantidade de Encaminhamentos'},
                        color='contagem'
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Mostrar tabela completa
                    with st.expander("Ver tabela completa de institui√ß√µes"):
                        st.dataframe(contagem_inst.reset_index(drop=True))
                else:
                    st.info(f"N√£o h√° encaminhamentos para institui√ß√µes do tipo '{tipo_inst_selecionado}' nos dados filtrados.")
            else:
                st.error(f"Coluna para o tipo de institui√ß√£o '{tipo_inst_selecionado}' n√£o encontrada nos dados.")
        else:
            st.info("N√£o h√° dados suficientes para an√°lise de encaminhamentos por articulador.")
    else:
        st.info("N√£o h√° dados suficientes para an√°lise de encaminhamentos por articulador.")
    
    # An√°lise temporal de encaminhamentos
    st.markdown("### An√°lise Temporal de Encaminhamentos")
    
    # Escolher visualiza√ß√£o: linha do tempo ou barras empilhadas
    tipo_visualizacao = st.radio(
        "Escolha o tipo de visualiza√ß√£o:",
        options=["Linha do Tempo", "Barras Empilhadas"],
        key="visualizacao_tipo_2"
    )
    
    # Agrupar por m√™s/ano
    if not df_filtrado.empty:
        # Criar Series com contagem de encaminhamentos por m√™s
        encaminhamentos_tempo = df_filtrado[df_filtrado['dado_algum_encaminhamento'] == 'Sim'].groupby('mes_ano').size()
        todos_registros_tempo = df_filtrado.groupby('mes_ano').size()
        
        # Criar DataFrame para visualiza√ß√£o
        df_tempo = pd.DataFrame({
            'total_registros': todos_registros_tempo,
            'com_encaminhamento': encaminhamentos_tempo
        }).fillna(0)
        
        # Adicionar percentual
        df_tempo['percentual'] = (df_tempo['com_encaminhamento'] / df_tempo['total_registros']) * 100
        
        # Adicionar display formatado do m√™s/ano
        df_tempo['mes_ano_display'] = [idx.split('-')[2] + ' ' + idx.split('-')[0] for idx in df_tempo.index]
        
        # Ordenar pelo √≠ndice (m√™s_ano) para garantir ordem cronol√≥gica
        df_tempo = df_tempo.sort_index()
        
        if not df_tempo.empty:
            # Criar visualiza√ß√£o conforme selecionado
            if tipo_visualizacao == "Linha do Tempo":
                # Gr√°fico de linha
                fig = px.line(
                    df_tempo.reset_index(),
                    x='mes_ano',
                    y=['com_encaminhamento', 'total_registros'],
                    title="Evolu√ß√£o de Encaminhamentos ao Longo do Tempo",
                    labels={'value': 'Quantidade', 'mes_ano': 'M√™s/Ano', 'variable': 'Tipo'},
                    color_discrete_map={
                        'com_encaminhamento': 'red',
                        'total_registros': 'blue'
                    }
                )
                
                # Customizar layout
                fig.update_layout(
                    xaxis_title="Per√≠odo",
                    yaxis_title="Quantidade de Registros",
                    legend_title="Tipo de Registro",
                    xaxis=dict(
                        tickmode='array',
                        tickvals=df_tempo.index,
                        ticktext=df_tempo['mes_ano_display']
                    )
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Gr√°fico de percentual
                fig_percentual = px.line(
                    df_tempo.reset_index(),
                    x='mes_ano',
                    y='percentual',
                    title="Percentual de Registros com Encaminhamento ao Longo do Tempo",
                    labels={'percentual': '% de Registros com Encaminhamento', 'mes_ano': 'M√™s/Ano'},
                    markers=True
                )
                
                # Customizar layout
                fig_percentual.update_layout(
                    xaxis_title="Per√≠odo",
                    yaxis_title="% de Registros com Encaminhamento",
                    xaxis=dict(
                        tickmode='array',
                        tickvals=df_tempo.index,
                        ticktext=df_tempo['mes_ano_display']
                    )
                )
                
                st.plotly_chart(fig_percentual, use_container_width=True)
                
            else:  # Barras empilhadas
                # Calcular registros sem encaminhamento
                df_tempo['sem_encaminhamento'] = df_tempo['total_registros'] - df_tempo['com_encaminhamento']
                
                # Gr√°fico de barras empilhadas
                fig = px.bar(
                    df_tempo.reset_index(),
                    x='mes_ano',
                    y=['com_encaminhamento', 'sem_encaminhamento'],
                    title="Distribui√ß√£o de Encaminhamentos ao Longo do Tempo",
                    labels={'value': 'Quantidade', 'mes_ano': 'M√™s/Ano', 'variable': 'Tipo de Registro'},
                    color_discrete_map={
                        'com_encaminhamento': '#1E90FF',
                        'sem_encaminhamento': '#D3D3D3'
                    }
                )
                
                # Customizar layout
                fig.update_layout(
                    xaxis_title="Per√≠odo",
                    yaxis_title="Quantidade de Registros",
                    legend_title="Status de Encaminhamento",
                    xaxis=dict(
                        tickmode='array',
                        tickvals=df_tempo.index,
                        ticktext=df_tempo['mes_ano_display']
                    )
                )
                
                st.plotly_chart(fig, use_container_width=True)
                
                # Gr√°fico de percentual
                fig_percentual = px.bar(
                    df_tempo.reset_index(),
                    x='mes_ano',
                    y='percentual',
                    title="Percentual de Registros com Encaminhamento ao Longo do Tempo",
                    labels={'percentual': '% de Registros com Encaminhamento', 'mes_ano': 'M√™s/Ano'},
                    color='percentual'
                )
                
                # Customizar layout
                fig_percentual.update_layout(
                    xaxis_title="Per√≠odo",
                    yaxis_title="% de Registros com Encaminhamento",
                    xaxis=dict(
                        tickmode='array',
                        tickvals=df_tempo.index,
                        ticktext=df_tempo['mes_ano_display']
                    )
                )
                
                st.plotly_chart(fig_percentual, use_container_width=True)
        else:
            st.info("N√£o h√° dados suficientes para an√°lise temporal de encaminhamentos.")
    else:
        st.info("N√£o h√° dados para an√°lise temporal de encaminhamentos.")

# TAB 8: MONITORAMENTO DE FOLLOW-UP DE ENCAMINHAMENTOS
elif tab_selecionada == "Monitoramento de Follow-up":
    st.subheader("Monitoramento de Follow-up de Encaminhamentos")
    
    # Adicionar explica√ß√£o sobre o monitoramento de follow-up
    with st.expander("üìã Entenda o Monitoramento de Follow-up", expanded=True):
        st.markdown("""
        ### O que √© o monitoramento de follow-up?
        
        O monitoramento de follow-up identifica encaminhamentos que n√£o tiveram follow-up (follow-up) ap√≥s terem sido realizados.
        √â importante verificar estes casos para garantir que o acompanhamento do cadastro est√° sendo feito de forma adequada.
        
        S√£o considerados sem seguimento os encaminhamentos que:
        
        1. Foram feitos para uma institui√ß√£o espec√≠fica
        2. J√° se passou um per√≠odo significativo desde o encaminhamento
        3. N√£o h√° registros posteriores mencionando a institui√ß√£o ou sinaliza√ß√µes de acompanhamento
        
        Essa an√°lise ajuda a identificar casos que precisam de aten√ß√£o e follow-up imediato, evitando que encaminhamentos sejam "perdidos" ou esquecidos.
        """)
    
    # Configura√ß√£o do n√∫mero de dias para considerar como sem follow-up
    dias_para_followup = st.slider(
        "Dias m√≠nimos para considerar que deveria haver follow-up", 
        min_value=7, 
        max_value=180, 
        value=30,
        key="dias_followup_slider"
    )
    
    # Bot√£o para verificar encaminhamentos sem follow-up
    if st.button("Verificar Encaminhamentos Sem Seguimento", key="btn_verificar_followup"):
        with st.spinner("Analisando encaminhamentos para identificar casos sem seguimento..."):
            # Detectar encaminhamentos sem follow-up
            df_sem_followup = detectar_encaminhamentos_sem_followup(df_filtrado, dias_para_followup)
            
            if not df_sem_followup.empty:
                # M√©tricas gerais
                total_sem_followup = len(df_sem_followup)
                total_criancas_afetadas = df_sem_followup['id_crianca'].nunique()
                
                st.subheader(f"Resultados: {total_sem_followup} Encaminhamentos Sem Seguimento Identificados")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Total de Encaminhamentos Sem Seguimento", total_sem_followup)
                
                with col2:
                    st.metric("Cadastros Afetados", total_criancas_afetadas)
                    
                with col3:
                    # M√©dia de dias desde o encaminhamento
                    media_dias = df_sem_followup['dias_desde_encaminhamento'].mean()
                    st.metric("M√©dia de Dias Sem Seguimento", f"{media_dias:.0f}")
                
                # Gr√°fico de distribui√ß√£o por articulador
                st.markdown("#### Distribui√ß√£o por Articulador")
                
                # Contar por articulador
                contagem_articulador = df_sem_followup['articulador'].value_counts().reset_index()
                contagem_articulador.columns = ['articulador', 'contagem']
                
                if len(contagem_articulador) > 0:
                    fig_articulador = px.bar(
                        contagem_articulador,
                        x='articulador',
                        y='contagem',
                        labels={'articulador': 'Articulador', 'contagem': 'Quantidade'},
                        title="Encaminhamentos Sem Seguimento por Articulador",
                        color='contagem'
                    )
                    
                    fig_articulador.update_layout(
                        xaxis_title="Articulador",
                        yaxis_title="Quantidade de Encaminhamentos Sem Seguimento"
                    )
                    
                    st.plotly_chart(fig_articulador, use_container_width=True)
                
                # Gr√°fico de distribui√ß√£o por intervalo de dias
                st.markdown("#### Distribui√ß√£o por Tempo Sem Seguimento")
                
                # Criar bins para agrupar por intervalos de dias
                bins = [0, 30, 60, 90, 120, 180, 365, 9999]
                labels = ['‚â§ 30 dias', '31-60 dias', '61-90 dias', '91-120 dias', '121-180 dias', '6-12 meses', '> 1 ano']
                
                df_sem_followup['intervalo_dias'] = pd.cut(
                    df_sem_followup['dias_desde_encaminhamento'], 
                    bins=bins, 
                    labels=labels, 
                    include_lowest=True
                )
                
                # Contar por intervalo
                contagem_intervalos = df_sem_followup['intervalo_dias'].value_counts().reset_index()
                contagem_intervalos.columns = ['intervalo', 'contagem']
                
                # Ordenar conforme os intervalos
                contagem_intervalos['intervalo'] = pd.Categorical(
                    contagem_intervalos['intervalo'], 
                    categories=labels, 
                    ordered=True
                )
                contagem_intervalos = contagem_intervalos.sort_values('intervalo')
                
                # Gr√°fico de barras
                if len(contagem_intervalos) > 0:
                    fig_intervalos = px.bar(
                        contagem_intervalos,
                        x='intervalo',
                        y='contagem',
                        labels={'intervalo': 'Intervalo de Dias', 'contagem': 'Quantidade'},
                        title="Distribui√ß√£o por Tempo Sem Seguimento",
                        color='intervalo',
                        text='contagem'
                    )
                    
                    st.plotly_chart(fig_intervalos, use_container_width=True)
                
                # Tabela detalhada com encaminhamentos sem follow-up
                st.markdown("#### Lista de Encaminhamentos Sem Seguimento")
                
                # Op√ß√£o para filtrar pela quantidade de dias
                dias_filtro = st.slider(
                    "Filtrar por dias m√≠nimos sem seguimento", 
                    min_value=int(df_sem_followup['dias_desde_encaminhamento'].min()), 
                    max_value=int(df_sem_followup['dias_desde_encaminhamento'].max()), 
                    value=30,
                    key="slider_filtro_dias_followup"
                )
                
                # Aplicar filtro
                df_filtrado_followup = df_sem_followup[df_sem_followup['dias_desde_encaminhamento'] >= dias_filtro]
                
                if not df_filtrado_followup.empty:
                    # Criar tabela mais amig√°vel para visualiza√ß√£o
                    df_visualizacao = df_filtrado_followup[['id_crianca', 'data_encaminhamento', 'dias_desde_encaminhamento', 
                                                            'articulador', 'instituicoes_encaminhadas']]
                    df_visualizacao = df_visualizacao.rename(columns={
                        'id_crianca': 'ID Cadastro',
                        'data_encaminhamento': 'Data do Encaminhamento',
                        'dias_desde_encaminhamento': 'Dias Sem Seguimento',
                        'articulador': 'Articulador',
                        'instituicoes_encaminhadas': 'Institui√ß√µes Encaminhadas'
                    })
                    
                    # Formatar a data
                    df_visualizacao['Data do Encaminhamento'] = df_visualizacao['Data do Encaminhamento'].dt.strftime('%d/%m/%Y')
                    
                    # Mostrar tabela com pagina√ß√£o
                    st.dataframe(df_visualizacao, use_container_width=True)
                    
                    # Expandir para ver detalhes de cada encaminhamento
                    st.markdown("#### Detalhes dos Encaminhamentos")
                    
                    # Limitar o n√∫mero de detalhes exibidos para melhor performance
                    max_detalhes = min(20, len(df_filtrado_followup))
                    
                    for i, (_, row) in enumerate(df_filtrado_followup.head(max_detalhes).iterrows()):
                        with st.expander(f"ID: {row['id_crianca']} | {row['dias_desde_encaminhamento']} dias sem seguimento | Articulador: {row['articulador']}"):
                            st.markdown(f"**ID do Cadastro:** {row['id_crianca']}")
                            st.markdown(f"**C√≥digo do Acompanhamento:** {row['acompanhamento_cod']}")
                            st.markdown(f"**Data do Encaminhamento:** {row['data_encaminhamento'].strftime('%d/%m/%Y')}")
                            st.markdown(f"**Institui√ß√µes Encaminhadas:** {row['instituicoes_encaminhadas']}")
                            st.markdown(f"**Descri√ß√£o do Encaminhamento:**")
                            st.text_area(f"descricao_encaminhamento_{i}", row['descricao'], height=100, disabled=True, label_visibility="collapsed")
                            
                            # Adicionar bot√£o para cada caso que sugere uma a√ß√£o
                            st.markdown("**A√ß√£o Recomendada:**")
                            st.markdown("Realizar contato com o cadastro para verificar status do encaminhamento e registrar o resultado.")
                else:
                    st.info(f"Nenhum encaminhamento sem seguimento por {dias_filtro} dias ou mais.")
            else:
                st.success("Parab√©ns! N√£o foram encontrados encaminhamentos sem seguimento no per√≠odo selecionado.")

# TAB 9: DETEC√á√ÉO DE DUPLICADOS
elif tab_selecionada == "Detec√ß√£o de Duplicados":
    st.subheader("Detec√ß√£o de Registros Duplicados")
    
    # Adicionar explica√ß√£o sobre a detec√ß√£o de duplicados
    with st.expander("üìã Entenda a Detec√ß√£o de Duplicados", expanded=True):
        st.markdown("""
        ### O que √© a detec√ß√£o de duplicados?
        
        A detec√ß√£o de duplicados identifica registros que possivelmente representam o mesmo caso ou trabalho, o que pode indicar problemas de registro, inconsist√™ncias ou duplica√ß√£o de esfor√ßos dos articuladores.
        
        Dois tipos de duplica√ß√£o s√£o analisados:
        
        1. **Duplica√ß√£o de Acompanhamentos**: Identifica registros com descri√ß√µes muito semelhantes para o mesmo cadastro.
        2. **Duplica√ß√£o de Encaminhamentos**: Identifica encaminhamentos feitos para as mesmas institui√ß√µes em um curto per√≠odo para o mesmo cadastro.
        
        Essa an√°lise ajuda a identificar casos que precisam de aten√ß√£o, revis√£o e poss√≠vel consolida√ß√£o.
        """)
    
    # Criar tabs para os dois tipos de detec√ß√£o
    dup_tab1, dup_tab2 = st.tabs(["Duplica√ß√£o de Acompanhamentos", "Duplica√ß√£o de Encaminhamentos"])
    
    # Tab de Duplica√ß√£o de Acompanhamentos
    with dup_tab1:
        st.markdown("### Detec√ß√£o de Acompanhamentos Duplicados")
        
        # Configura√ß√£o do threshold de similaridade para textos
        threshold_texto = st.slider(
            "Threshold de Similaridade (0-1):",
            min_value=0.5,
            max_value=1.0,
            value=0.7,
            step=0.05,
            help="Quanto maior o valor, mais semelhantes os textos precisam ser para serem considerados duplicados",
            key="threshold_texto_slider"
        )
        
        # Bot√£o para calcular similaridade entre descri√ß√µes
        if st.button("Detectar Acompanhamentos Duplicados", key="btn_calcular_similaridade"):
            with st.spinner("Calculando similaridade entre descri√ß√µes de acompanhamentos..."):
                # Calcular similaridade entre descri√ß√µes
                df_similaridade = calcular_similaridade_descricoes(df_filtrado)
                
                if not df_similaridade.empty:
                    # Filtrar registros baseado no threshold
                    df_duplicados = df_similaridade[df_similaridade['similaridade'] >= threshold_texto]
                    
                    if not df_duplicados.empty:
                        # M√©tricas
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Total de Duplica√ß√µes Potenciais", len(df_duplicados))
                        with col2:
                            st.metric("Similaridade M√©dia", f"{df_duplicados['similaridade'].mean():.2f}")
                        with col3:
                            if 'id_crianca' in df_duplicados.columns:
                                n_beneficiarios = df_duplicados['id_crianca'].nunique()
                                st.metric("Cadastros Afetados", n_beneficiarios)
                        
                        # Mostrar detalhes dos duplicados
                        st.markdown("#### Detalhes dos Duplicados")
                        
                        for i, (_, row) in enumerate(df_duplicados.iterrows()):
                            with st.expander(f"Par {i+1}: Similaridade {row['similaridade']:.2f} | Diferen√ßa: {row['diferenca_dias']} dias"):
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.markdown(f"**ID do Cadastro:** {row['id_crianca'] if 'id_crianca' in row else 'N/A'}")
                                    st.markdown(f"**C√≥digo do Acompanhamento:** {row['acompanhamento_cod_1'] if 'acompanhamento_cod_1' in row else row['id_registro_1']}")
                                    st.markdown(f"**Data:** {row['data_registro_1'].strftime('%d/%m/%Y')}")
                                    st.markdown(f"**Articulador:** {row['articulador_1']}")
                                    st.text_area("Descri√ß√£o 1", row['descricao_1'], height=150, key=f"desc1_{i}")
                                
                                with col2:
                                    st.markdown(f"**ID do Cadastro:** {row['id_crianca'] if 'id_crianca' in row else 'N/A'}")
                                    st.markdown(f"**C√≥digo do Acompanhamento:** {row['acompanhamento_cod_2'] if 'acompanhamento_cod_2' in row else row['id_registro_2']}")
                                    st.markdown(f"**Data:** {row['data_registro_2'].strftime('%d/%m/%Y')}")
                                    st.markdown(f"**Articulador:** {row['articulador_2']}")
                                    st.text_area("Descri√ß√£o 2", row['descricao_2'], height=150, key=f"desc2_{i}")
                    else:
                        st.success("√ìtimo! N√£o foram encontrados acompanhamentos duplicados com o threshold selecionado.")
                else:
                    st.info("N√£o h√° registros suficientes para an√°lise ou as condi√ß√µes para detec√ß√£o n√£o foram atendidas.")
    
    # Tab de Duplica√ß√£o de Encaminhamentos
    with dup_tab2:
        st.markdown("### Detec√ß√£o de Encaminhamentos Duplicados")
        
        # Configura√ß√£o do per√≠odo para considerar duplicidade
        dias_limite = st.slider(
            "Janela de tempo para considerar duplicidade (dias):",
            min_value=1,
            max_value=90,
            value=30,
            step=1,
            help="Encaminhamentos para a mesma institui√ß√£o dentro deste per√≠odo ser√£o analisados",
            key="dias_limite_slider"
        )
        
        # Bot√£o para detectar encaminhamentos duplicados
        if st.button("Detectar Encaminhamentos Duplicados", key="btn_detectar_enc_dup"):
            with st.spinner("Analisando encaminhamentos para identificar poss√≠veis duplica√ß√µes..."):
                # Detectar encaminhamentos duplicados
                df_duplicacoes = detectar_encaminhamentos_duplicados(df_filtrado, dias_limite)
                
                if not df_duplicacoes.empty:
                    # M√©tricas
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Total de Encaminhamentos Duplicados", len(df_duplicacoes))
                    with col2:
                        media_dias = df_duplicacoes['diferenca_dias'].mean()
                        st.metric("M√©dia de Dias Entre Duplica√ß√µes", f"{media_dias:.1f}")
                    with col3:
                        n_beneficiarios = df_duplicacoes['id_crianca'].nunique()
                        st.metric("Cadastros Afetados", n_beneficiarios)
                    
                    # Gr√°fico de articuladores envolvidos
                    st.markdown("#### Articuladores Envolvidos nas Duplica√ß√µes")
                    
                    # Criar DataFrame para articuladores
                    # Considerar tanto o articulador_1 quanto o articulador_2
                    articuladores = pd.concat([
                        pd.DataFrame({'articulador': df_duplicacoes['articulador_1']}),
                        pd.DataFrame({'articulador': df_duplicacoes['articulador_2']})
                    ])
                    
                    contagem_articuladores = articuladores['articulador'].value_counts().reset_index()
                    contagem_articuladores.columns = ['articulador', 'contagem']
                    
                    fig = px.bar(
                        contagem_articuladores,
                        x='articulador',
                        y='contagem',
                        title="Articuladores Envolvidos em Encaminhamentos Duplicados",
                        color='contagem'
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Detalhes dos duplicados
                    st.markdown("#### Detalhes dos Encaminhamentos Duplicados")
                    
                    for i, (_, row) in enumerate(df_duplicacoes.iterrows()):
                        with st.expander(f"Par {i+1}: ID Cadastro {row['id_crianca']} | Diferen√ßa: {row['diferenca_dias']} dias | Institui√ß√µes: {row['instituicoes_comuns']}"):
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                st.markdown(f"**ID do Cadastro:** {row['id_crianca']}")
                                st.markdown(f"**C√≥digo do Acompanhamento:** {row['acompanhamento_cod_1'] if 'acompanhamento_cod_1' in row else row['id_registro_1']}")
                                st.markdown(f"**Data:** {row['data_registro_1'].strftime('%d/%m/%Y')}")
                                st.markdown(f"**Articulador:** {row['articulador_1']}")
                                st.text_area("Descri√ß√£o 1", row['descricao_1'], height=150, key=f"enc_desc1_{i}")
                            
                            with col2:
                                st.markdown(f"**ID do Cadastro:** {row['id_crianca']}")
                                st.markdown(f"**C√≥digo do Acompanhamento:** {row['acompanhamento_cod_2'] if 'acompanhamento_cod_2' in row else row['id_registro_2']}")
                                st.markdown(f"**Data:** {row['data_registro_2'].strftime('%d/%m/%Y')}")
                                st.markdown(f"**Articulador:** {row['articulador_2']}")
                                st.text_area("Descri√ß√£o 2", row['descricao_2'], height=150, key=f"enc_desc2_{i}")
                            
                            st.markdown(f"**Mesmo Articulador:** {'Sim' if row['mesmo_articulador'] else 'N√£o'}")
                            st.markdown(f"**Institui√ß√µes Comuns:** {row['instituicoes_comuns']}")
                else:
                    st.success("√ìtimo! N√£o foram encontrados encaminhamentos duplicados no per√≠odo analisado.")

# Mostrar dados brutos (opcional)
with st.expander("Ver Dados Brutos"):
    # Configurar op√ß√µes para a tabela AgGrid
    gb = GridOptionsBuilder.from_dataframe(df_filtrado)
    gb.configure_pagination(paginationAutoPageSize=False, paginationPageSize=15)
    gb.configure_selection('multiple', use_checkbox=True, groupSelectsChildren=True)
    gb.configure_default_column(groupable=True, value=True, enableRowGroup=True, aggFunc='sum', editable=True)
    
    # Personalizar colunas espec√≠ficas
    gb.configure_column('acompanhamento_articulador', header_name='Articulador', width=150)
    gb.configure_column('acompanhamento_data', header_name='Data', type=["dateColumnFilter"], width=120)
    gb.configure_column('classificacao_qualidade', header_name='Qualidade', width=120)
    gb.configure_column('pontuacao', header_name='Pontua√ß√£o', type=["numericColumn", "numberColumnFilter"], width=100)
    gb.configure_column('acompanhamento_descricao', header_name='Descri√ß√£o', width=300)
    
    # Adicionar filtros e outras funcionalidades
    gb.configure_grid_options(domLayout='normal', enableRangeSelection=True)
    
    # Construir as op√ß√µes da grid
    grid_options = gb.build()
    
    # Exibir a tabela interativa com AgGrid
    st.write("#### Dados Filtrados - Tabela Interativa")
    st.markdown("""
    <style>
    .ag-header-cell-text {
        font-weight: bold;
        color: #1E88E5;
    }
    </style>
    """, unsafe_allow_html=True)
    
    grid_response = AgGrid(
        df_filtrado,
        gridOptions=grid_options,
        data_return_mode=DataReturnMode.FILTERED_AND_SORTED,
        update_mode=GridUpdateMode.MODEL_CHANGED,
        fit_columns_on_grid_load=False,
        theme='streamlit',
        height=500,
        allow_unsafe_jscode=True,
        enable_enterprise_modules=False
    )
    
    # Exibir informa√ß√µes sobre os dados selecionados (opcional)
    selected = grid_response['selected_rows']
    if selected:
        st.write(f"##### Registros Selecionados: {len(selected)}")
        st.json(selected) 

# Adicionar se√ß√£o de exporta√ß√£o de relat√≥rios
st.sidebar.markdown("---")
st.sidebar.header("Exportar Relat√≥rio")

# Op√ß√µes de relat√≥rio
tipo_relatorio = st.sidebar.selectbox(
    "Tipo de Relat√≥rio",
    ["Resumo Geral", 
     "An√°lise de Qualidade", 
     "Consist√™ncia de Sucesso", 
     "Encaminhamentos sem Follow-up",
     "Registros Duplicados",
     "Dados Completos"]
)

# Formato de exporta√ß√£o
formato_exportacao = st.sidebar.selectbox(
    "Formato",
    ["Excel (.xlsx)", "CSV (.csv)"]
)

# Bot√£o para gerar o relat√≥rio
if st.sidebar.button("Gerar Relat√≥rio"):
    with st.spinner("Gerando relat√≥rio..."):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Definir caminho de arquivo com base no formato selecionado
        extensao = ".xlsx" if "Excel" in formato_exportacao else ".csv"
        nome_arquivo = f"relatorio_{tipo_relatorio.lower().replace(' ', '_')}_{timestamp}{extensao}"
        
        # Criar diferentes relat√≥rios baseados na sele√ß√£o
        if tipo_relatorio == "Resumo Geral":
            # Criar DataFrame de resumo
            resumo_data = {
                "M√©trica": [
                    "Total de Registros", 
                    "Registros de Alta Qualidade (%)",
                    "Registros com Problemas (%)",
                    "Registros com Inconsist√™ncias (%)",
                    "Registros com Encaminhamentos (%)",
                    "Registros sem Follow-up (%)"
                ],
                "Valor": [
                    len(df_filtrado),
                    (df_filtrado['classificacao_qualidade'].isin(['Excelente', 'Bom']).mean() * 100) if 'classificacao_qualidade' in df_filtrado.columns else 0,
                    (df_filtrado['num_problemas'] > 0).mean() * 100 if 'num_problemas' in df_filtrado.columns else 0,
                    (df_filtrado['consistencia_sucesso'].str.contains('Inconsist√™ncia').mean() * 100) if 'consistencia_sucesso' in df_filtrado.columns else 0,
                    (df_filtrado['dado_algum_encaminhamento'] == 'Sim').mean() * 100 if 'dado_algum_encaminhamento' in df_filtrado.columns else 0,
                    0  # Placeholder para taxa de n√£o-followup
                ]
            }
            
            df_resumo = pd.DataFrame(resumo_data)
            
            # Adicionar resumo por articulador
            if 'acompanhamento_articulador' in df_filtrado.columns:
                resumo_articulador = df_filtrado.groupby('acompanhamento_articulador').agg({
                    'acompanhamento_descricao': 'count',
                    'pontuacao': 'mean',
                    'num_problemas': lambda x: (x > 0).mean() * 100
                }).reset_index()
                
                resumo_articulador.columns = ['Articulador', 'Total de Registros', 'Pontua√ß√£o M√©dia', '% com Problemas']
                
                # Formatar n√∫meros
                resumo_articulador['Pontua√ß√£o M√©dia'] = resumo_articulador['Pontua√ß√£o M√©dia'].round(2)
                resumo_articulador['% com Problemas'] = resumo_articulador['% com Problemas'].round(2)
                
                # Exportar para o formato selecionado
                if "Excel" in formato_exportacao:
                    with pd.ExcelWriter(nome_arquivo) as writer:
                        df_resumo.to_excel(writer, sheet_name='Resumo Geral', index=False)
                        resumo_articulador.to_excel(writer, sheet_name='Resumo por Articulador', index=False)
                        
                        # Adicionar informa√ß√µes sobre os filtros aplicados
                        pd.DataFrame({
                            'Filtro': ['Per√≠odo Inicial', 'Per√≠odo Final', 'Articulador', 'Status Sucesso', 'Qualidade'],
                            'Valor': [
                                data_inicio.strftime('%d/%m/%Y'), 
                                data_fim.strftime('%d/%m/%Y'),
                                articulador_selecionado if articulador_selecionado != 'Todos' else 'Todos',
                                sucesso_selecionado if sucesso_selecionado != 'Todos' else 'Todos',
                                qualidade_selecionada if qualidade_selecionada != 'Todas' else 'Todas'
                            ]
                        }).to_excel(writer, sheet_name='Filtros Aplicados', index=False)
                else:
                    df_resumo.to_csv(nome_arquivo, index=False)
            
        elif tipo_relatorio == "An√°lise de Qualidade":
            # An√°lise detalhada de qualidade
            if 'classificacao_qualidade' in df_filtrado.columns and 'problemas_detectados' in df_filtrado.columns:
                # Distribui√ß√£o de classifica√ß√£o de qualidade
                qualidade_counts = df_filtrado['classificacao_qualidade'].value_counts().reset_index()
                qualidade_counts.columns = ['Classifica√ß√£o', 'Quantidade']
                
                # Problemas detectados
                problemas_lista = []
                for problemas in df_filtrado['problemas_detectados']:
                    problemas_lista.extend(problemas)
                
                problemas_counts = pd.Series(problemas_lista).value_counts().reset_index()
                problemas_counts.columns = ['Tipo de Problema', 'Ocorr√™ncias']
                
                # Exportar para o formato selecionado
                if "Excel" in formato_exportacao:
                    with pd.ExcelWriter(nome_arquivo) as writer:
                        qualidade_counts.to_excel(writer, sheet_name='Distribui√ß√£o de Qualidade', index=False)
                        problemas_counts.to_excel(writer, sheet_name='Problemas Detectados', index=False)
                        
                        # Adicionar exemplos de problemas para cada categoria
                        for problema in problemas_counts['Tipo de Problema'].unique():
                            exemplos = df_filtrado[df_filtrado['problemas_detectados'].apply(lambda x: problema in x)]
                            if not exemplos.empty:
                                exemplos = exemplos[['id', 'acompanhamento_cod', 'acompanhamento_articulador', 
                                                    'acompanhamento_data', 'acompanhamento_descricao', 'pontuacao']]
                                exemplos = exemplos.head(10)  # Limitar a 10 exemplos
                                exemplos.columns = ['ID Cadastro', 'C√≥digo Acompanhamento', 'Articulador', 
                                                   'Data', 'Descri√ß√£o', 'Pontua√ß√£o']
                                exemplos.to_excel(writer, sheet_name=f'Problema_{problema[:28]}', index=False)
                else:
                    pd.concat([qualidade_counts, pd.DataFrame(), problemas_counts], axis=0).to_csv(nome_arquivo, index=False)
            
        elif tipo_relatorio == "Consist√™ncia de Sucesso":
            # Relat√≥rio de inconsist√™ncias de sucesso
            if 'consistencia_sucesso' in df_filtrado.columns:
                df_inconsistente = df_filtrado[df_filtrado['consistencia_sucesso'].isin(
                    ['Poss√≠vel Inconsist√™ncia (Sim/Insucesso)', 'Poss√≠vel Inconsist√™ncia (N√£o/Sucesso)'])]
                
                if not df_inconsistente.empty:
                    # Adicionar tipo de inconsist√™ncia
                    df_inconsistente['tipo_inconsistencia'] = df_inconsistente.apply(
                        lambda row: "Marcado como 'Sim' mas descri√ß√£o indica insucesso" if row['acompanhamento_sucesso_contato'] == 'Sim'
                        else "Marcado como 'N√£o' mas descri√ß√£o indica sucesso", axis=1
                    )
                    
                    # Resumo das inconsist√™ncias
                    resumo_inconsistencia = pd.DataFrame({
                        'M√©trica': [
                            'Total de Registros Analisados',
                            'Registros com Inconsist√™ncias',
                            'Falsos Sucessos',
                            'Falsos Insucessos'
                        ],
                        'Valor': [
                            len(df_filtrado),
                            len(df_inconsistente),
                            len(df_inconsistente[df_inconsistente['tipo_inconsistencia'] == "Marcado como 'Sim' mas descri√ß√£o indica insucesso"]),
                            len(df_inconsistente[df_inconsistente['tipo_inconsistencia'] == "Marcado como 'N√£o' mas descri√ß√£o indica sucesso"])
                        ]
                    })
                    
                    # Detalhes dos registros inconsistentes
                    detalhes_inconsistencia = df_inconsistente[[
                        'id', 'acompanhamento_cod', 'acompanhamento_articulador', 'acompanhamento_data', 
                        'acompanhamento_sucesso_contato', 'tipo_inconsistencia', 'acompanhamento_descricao'
                    ]].copy()
                    
                    # Renomear colunas
                    detalhes_inconsistencia.columns = [
                        'ID Cadastro', 'C√≥digo Acompanhamento', 'Articulador', 'Data', 
                        'Status Reportado', 'Tipo de Inconsist√™ncia', 'Descri√ß√£o'
                    ]
                    
                    # Exportar para o formato selecionado
                    if "Excel" in formato_exportacao:
                        with pd.ExcelWriter(nome_arquivo) as writer:
                            resumo_inconsistencia.to_excel(writer, sheet_name='Resumo Inconsist√™ncias', index=False)
                            detalhes_inconsistencia.to_excel(writer, sheet_name='Detalhes Inconsist√™ncias', index=False)
                    else:
                        detalhes_inconsistencia.to_csv(nome_arquivo, index=False)
                else:
                    # Se n√£o houver inconsist√™ncias, criar um relat√≥rio simples
                    pd.DataFrame({'Resultado': ['N√£o foram encontradas inconsist√™ncias nos registros filtrados.']}).to_csv(nome_arquivo, index=False)
            
        elif tipo_relatorio == "Encaminhamentos sem Follow-up":
            # Detectar encaminhamentos sem follow-up
            df_sem_followup = detectar_encaminhamentos_sem_followup(df_filtrado, 30)
            
            if not df_sem_followup.empty:
                # Preparar dados para exporta√ß√£o
                df_export = df_sem_followup[[
                    'id_crianca', 'acompanhamento_cod', 'data_encaminhamento', 'dias_desde_encaminhamento',
                    'articulador', 'instituicoes_encaminhadas', 'descricao'
                ]].copy()
                
                # Renomear colunas
                df_export.columns = [
                    'ID Cadastro', 'C√≥digo Acompanhamento', 'Data Encaminhamento', 'Dias Sem Seguimento',
                    'Articulador', 'Institui√ß√µes Encaminhadas', 'Descri√ß√£o'
                ]
                
                # Exportar para o formato selecionado
                if "Excel" in formato_exportacao:
                    with pd.ExcelWriter(nome_arquivo) as writer:
                        df_export.to_excel(writer, sheet_name='Encaminhamentos sem Follow-up', index=False)
                        
                        # Adicionar um resumo
                        pd.DataFrame({
                            'M√©trica': [
                                'Total de Encaminhamentos sem Follow-up',
                                'Cadastros Afetados',
                                'M√©dia de Dias sem Seguimento'
                            ],
                            'Valor': [
                                len(df_sem_followup),
                                df_sem_followup['id_crianca'].nunique(),
                                df_sem_followup['dias_desde_encaminhamento'].mean()
                            ]
                        }).to_excel(writer, sheet_name='Resumo', index=False)
                else:
                    df_export.to_csv(nome_arquivo, index=False)
            else:
                # Se n√£o houver encaminhamentos sem follow-up, criar um relat√≥rio simples
                pd.DataFrame({'Resultado': ['N√£o foram encontrados encaminhamentos sem follow-up.']}).to_csv(nome_arquivo, index=False)
            
        elif tipo_relatorio == "Registros Duplicados":
            # Detectar registros duplicados
            df_duplicados = detectar_encaminhamentos_duplicados(df_filtrado, 30)
            
            if not df_duplicados.empty:
                # Preparar dados para exporta√ß√£o
                df_export = df_duplicados[[
                    'id_crianca', 'id_registro_1', 'id_registro_2', 'acompanhamento_cod_1', 'acompanhamento_cod_2',
                    'data_registro_1', 'data_registro_2', 'diferenca_dias', 'articulador_1', 'articulador_2',
                    'mesmo_articulador', 'instituicoes_comuns'
                ]].copy()
                
                # Renomear colunas
                df_export.columns = [
                    'ID Cadastro', 'ID Registro 1', 'ID Registro 2', 'C√≥digo Acompanhamento 1', 'C√≥digo Acompanhamento 2',
                    'Data 1', 'Data 2', 'Diferen√ßa (dias)', 'Articulador 1', 'Articulador 2',
                    'Mesmo Articulador', 'Institui√ß√µes Comuns'
                ]
                
                # Exportar para o formato selecionado
                if "Excel" in formato_exportacao:
                    with pd.ExcelWriter(nome_arquivo) as writer:
                        df_export.to_excel(writer, sheet_name='Encaminhamentos Duplicados', index=False)
                        
                        # Adicionar descri√ß√µes (em outra planilha para facilitar visualiza√ß√£o)
                        descricoes = df_duplicados[['id_crianca', 'descricao_1', 'descricao_2']].copy()
                        descricoes.columns = ['ID Cadastro', 'Descri√ß√£o 1', 'Descri√ß√£o 2']
                        descricoes.to_excel(writer, sheet_name='Descri√ß√µes', index=False)
                else:
                    df_export.to_csv(nome_arquivo, index=False)
            else:
                # Se n√£o houver duplicados, criar um relat√≥rio simples
                pd.DataFrame({'Resultado': ['N√£o foram encontrados registros duplicados.']}).to_csv(nome_arquivo, index=False)
        
        else:  # Dados Completos
            # Exportar todos os dados filtrados
            if "Excel" in formato_exportacao:
                df_filtrado.to_excel(nome_arquivo, index=False)
            else:
                df_filtrado.to_csv(nome_arquivo, index=False)
        
        # Mostrar link para download
        st.sidebar.success(f"Relat√≥rio gerado com sucesso!")
        
        # Criar link para download
        with open(nome_arquivo, "rb") as file:
            st.sidebar.download_button(
                label="Baixar Relat√≥rio",
                data=file,
                file_name=nome_arquivo,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet" if "Excel" in formato_exportacao else "text/csv"
            )
            
        # Mostrar informa√ß√µes sobre o relat√≥rio
        st.sidebar.info(f"O relat√≥rio cont√©m dados de {len(df_filtrado)} registros, aplicando todos os filtros selecionados.")